{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Required Packages**"
      ],
      "metadata": {
        "id": "io6Xw6UbXqV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install biopython pandas numpy requests python-dotenv\n",
        "\n",
        "# For medical NLP\n",
        "!pip install spacy scispacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vlBEHOO7VWFR",
        "outputId": "3bceba44-c837-4f41-d14c-30fb736edb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Collecting scispacy\n",
            "  Downloading scispacy-0.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from scispacy) (1.16.2)\n",
            "Collecting conllu (from scispacy)\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scispacy) (1.5.2)\n",
            "Collecting nmslib-metabrainz==2.1.3 (from scispacy)\n",
            "  Downloading nmslib_metabrainz-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (956 bytes)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.12/dist-packages (from scispacy) (1.6.1)\n",
            "Collecting pysbd (from scispacy)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pybind11>=2.2.3 (from nmslib-metabrainz==2.1.3->scispacy)\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading scispacy-0.6.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nmslib_metabrainz-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pysbd, pybind11, numpy, conllu, nmslib-metabrainz, blis, thinc, scispacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.2.1 conllu-6.0.0 nmslib-metabrainz-2.1.3 numpy-1.26.4 pybind11-3.0.1 pysbd-0.3.4 scispacy-0.6.2 thinc-8.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5a64e27d27db4400a7171b028fc7b45d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Collecting Data**"
      ],
      "metadata": {
        "id": "YkpveqXqYu_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgyuMN04VDpe"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Set your email\n",
        "Entrez.email = \"meghsuhanths2306@gmail.com\"\n",
        "\n",
        "def collect_pubmed_fixed(search_term, max_results=100):\n",
        "    \"\"\"\n",
        "    Fixed PubMED collection with proper XML handling\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Collecting articles for: {search_term}\")\n",
        "\n",
        "\n",
        "    # Step 1: Search for PMIDs\n",
        "    try:\n",
        "        handle = Entrez.esearch(\n",
        "            db=\"pubmed\",\n",
        "            term=search_term,\n",
        "            retmax=max_results,\n",
        "            sort=\"relevance\"\n",
        "        )\n",
        "\n",
        "        results = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        pmids = results['IdList']\n",
        "        total_count = int(results['Count'])\n",
        "\n",
        "        print(f\"Found {total_count} total articles\")\n",
        "        print(f\"Retrieving {len(pmids)} articles\\n\")\n",
        "\n",
        "        if len(pmids) == 0:\n",
        "            print(\"No PMIDs found!\")\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Step 2: Fetch abstracts - FIXED VERSION\n",
        "    print(\"Step 2: Fetching article details...\")\n",
        "    articles = []\n",
        "    batch_size = 50\n",
        "\n",
        "    for i in range(0, len(pmids), batch_size):\n",
        "        batch_pmids = pmids[i:i+batch_size]\n",
        "        batch_num = (i // batch_size) + 1\n",
        "        total_batches = (len(pmids) + batch_size - 1) // batch_size\n",
        "\n",
        "        print(f\"  Fetching batch {batch_num}/{total_batches} ({len(batch_pmids)} articles)...\")\n",
        "\n",
        "        try:\n",
        "            # FIX: Use rettype=\"abstract\" and retmode=\"xml\"\n",
        "            handle = Entrez.efetch(\n",
        "                db=\"pubmed\",\n",
        "                id=batch_pmids,\n",
        "                rettype=\"abstract\",\n",
        "                retmode=\"xml\"\n",
        "            )\n",
        "\n",
        "            # FIX: Read the XML properly\n",
        "            records = Entrez.read(handle)\n",
        "            handle.close()\n",
        "\n",
        "            # Process each article\n",
        "            for record in records['PubmedArticle']:\n",
        "                try:\n",
        "                    medline_citation = record['MedlineCitation']\n",
        "                    article = medline_citation['Article']\n",
        "\n",
        "                    # Extract PMID\n",
        "                    pmid = str(medline_citation['PMID'])\n",
        "\n",
        "                    # Extract title\n",
        "                    title = article.get('ArticleTitle', '')\n",
        "\n",
        "                    # Extract abstract\n",
        "                    abstract = ''\n",
        "                    if 'Abstract' in article:\n",
        "                        abstract_texts = article['Abstract'].get('AbstractText', [])\n",
        "                        # Join all abstract parts\n",
        "                        abstract = ' '.join([str(text) for text in abstract_texts])\n",
        "\n",
        "                    # Extract journal\n",
        "                    journal = article.get('Journal', {}).get('Title', '')\n",
        "\n",
        "                    # Extract publication date\n",
        "                    pub_date = ''\n",
        "                    if 'Journal' in article and 'JournalIssue' in article['Journal']:\n",
        "                        pub_info = article['Journal']['JournalIssue'].get('PubDate', {})\n",
        "                        year = pub_info.get('Year', '')\n",
        "                        month = pub_info.get('Month', '')\n",
        "                        pub_date = f\"{year}-{month}\" if year else ''\n",
        "\n",
        "                    # Extract authors\n",
        "                    authors = []\n",
        "                    if 'AuthorList' in article:\n",
        "                        for author in article['AuthorList']:\n",
        "                            if 'LastName' in author:\n",
        "                                name = f\"{author.get('LastName', '')} {author.get('Initials', '')}\"\n",
        "                                authors.append(name.strip())\n",
        "\n",
        "                    # Only keep if we have an abstract\n",
        "                    if abstract and len(abstract) > 50:\n",
        "                        article_data = {\n",
        "                            'pmid': pmid,\n",
        "                            'title': title,\n",
        "                            'abstract': abstract,\n",
        "                            'journal': journal,\n",
        "                            'publication_date': pub_date,\n",
        "                            'authors': ', '.join(authors),\n",
        "                            'num_authors': len(authors),\n",
        "                            'abstract_length': len(abstract)\n",
        "                        }\n",
        "                        articles.append(article_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Warning: Skipped one article: {str(e)[:50]}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"  Collected {len(articles)} articles so far\")\n",
        "\n",
        "            # Rate limiting\n",
        "            time.sleep(0.4)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Batch {batch_num} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n Collection complete: {len(articles)} articles with abstracts\\n\")\n",
        "    return articles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just testing with small data\n",
        "\n",
        "print(\"TESTING DATA COLLECTION\")\n",
        "\n",
        "# collecting 50 diabetes articles\n",
        "test_data = collect_pubmed_fixed(\"diabetes\", max_results=50)\n",
        "\n",
        "if len(test_data) > 0:\n",
        "    print(f\"Collected {len(test_data)} articles\\n\")\n",
        "\n",
        "    print(\"Sample Article:\")\n",
        "    print(f\"PMID: {test_data[0]['pmid']}\")\n",
        "    print(f\"Title: {test_data[0]['title']}\")\n",
        "    print(f\"Journal: {test_data[0]['journal']}\")\n",
        "    print(f\"Authors: {test_data[0]['authors'][:80]}...\")\n",
        "    print(f\"Abstract length: {test_data[0]['abstract_length']} characters\")\n",
        "    print(f\"\\nAbstract preview:\")\n",
        "    print(test_data[0]['abstract'][:300] + \"...\\n\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(test_data)\n",
        "    print(f\"DataFrame shape: {df.shape}\")\n",
        "    print(f\"\\nDataFrame columns: {list(df.columns)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n No articles collected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4yrjtrxZtB2",
        "outputId": "35696c0e-a4bd-4893-f1c7-0e889daafa49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING DATA COLLECTION\n",
            "Collecting articles for: diabetes\n",
            "Found 1078236 total articles\n",
            "Retrieving 50 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/1 (50 articles)...\n",
            "  Collected 38 articles so far\n",
            "\n",
            " Collection complete: 38 articles with abstracts\n",
            "\n",
            "Collected 38 articles\n",
            "\n",
            "Sample Article:\n",
            "PMID: 32741486\n",
            "Title: Diabetes Insipidus: An Update.\n",
            "Journal: Endocrinology and metabolism clinics of North America\n",
            "Authors: Refardt J, Winzeler B, Christ-Crain M...\n",
            "Abstract length: 775 characters\n",
            "\n",
            "Abstract preview:\n",
            "The differential diagnosis of diabetes insipidus involves the distinction between central or nephrogenic diabetes insipidus and primary polydipsia. Differentiation is important because treatment strategies vary; the wrong treatment can be dangerous. Reliable differentiation is difficult especially i...\n",
            "\n",
            "DataFrame shape: (38, 8)\n",
            "\n",
            "DataFrame columns: ['pmid', 'title', 'abstract', 'journal', 'publication_date', 'authors', 'num_authors', 'abstract_length']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_medical_specialties(max_per_specialty=10000):\n",
        "    \"\"\"\n",
        "    Collect data for all medical specialties\n",
        "    \"\"\"\n",
        "\n",
        "    # medical specialties\n",
        "    specialties = {\n",
        "        'cardiology': 'cardiology AND 2020:2024[DP] AND English[LA]',\n",
        "        'diabetes': 'diabetes mellitus AND 2020:2024[DP] AND English[LA]',\n",
        "        'infectious_diseases': 'infectious diseases AND 2020:2024[DP] AND English[LA]'\n",
        "    }\n",
        "\n",
        "    all_articles = []\n",
        "\n",
        "    for specialty_name, query in specialties.items():\n",
        "        print(f\"COLLECTING: {specialty_name.upper()}\")\n",
        "\n",
        "        # Collect articles\n",
        "        articles = collect_pubmed_fixed(query, max_results=max_per_specialty)\n",
        "\n",
        "        # Add specialty label\n",
        "        for article in articles:\n",
        "            article['specialty'] = specialty_name\n",
        "            article['collection_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        all_articles.extend(articles)\n",
        "\n",
        "        print(f\"\\n {specialty_name}: {len(articles)} articles collected\")\n",
        "        print(f\"Total so far: {len(all_articles)} articles\\n\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_df = pd.DataFrame(articles)\n",
        "        checkpoint_df.to_csv(f'data_{specialty_name}_checkpoint.csv', index=False)\n",
        "        print(f\"Checkpoint saved: data_{specialty_name}_checkpoint.csv\")\n",
        "\n",
        "        # Wait between specialties\n",
        "        time.sleep(2)\n",
        "\n",
        "    return all_articles"
      ],
      "metadata": {
        "id": "BOXFGroyVIew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "automatically saves checkpoints. So, we can stop whenever we want"
      ],
      "metadata": {
        "id": "gC1KBehkasei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Full collection for 3 specialities\n",
        "medical_data = collect_medical_specialties(max_per_specialty=1000)  # with 1K per specialty\n",
        "\n",
        "df_final = pd.DataFrame(medical_data)\n",
        "\n",
        "# Save final dataset\n",
        "df_final.to_csv('medical_literature_dataset.csv', index=False)\n",
        "df_final.to_json('medical_literature_dataset.json', orient='records', indent=2)\n",
        "\n",
        "print(f\"Total articles: {len(df_final)}\")\n",
        "print(f\"Specialties: {df_final['specialty'].value_counts().to_dict()}\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(\"  - medical_literature_dataset.csv\")\n",
        "print(\"  - medical_literature_dataset.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIJpnmAabUN",
        "outputId": "ea3a33a9-53bb-4283-c04f-9b525105bd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COLLECTING: CARDIOLOGY\n",
            "Collecting articles for: cardiology AND 2020:2024[DP] AND English[LA]\n",
            "Found 222699 total articles\n",
            "Retrieving 1000 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/20 (50 articles)...\n",
            "  Collected 32 articles so far\n",
            "  Fetching batch 2/20 (50 articles)...\n",
            "  Collected 61 articles so far\n",
            "  Fetching batch 3/20 (50 articles)...\n",
            "  Collected 91 articles so far\n",
            "  Fetching batch 4/20 (50 articles)...\n",
            "  Collected 120 articles so far\n",
            "  Fetching batch 5/20 (50 articles)...\n",
            "  Collected 144 articles so far\n",
            "  Fetching batch 6/20 (50 articles)...\n",
            "  Collected 169 articles so far\n",
            "  Fetching batch 7/20 (50 articles)...\n",
            "  Collected 190 articles so far\n",
            "  Fetching batch 8/20 (50 articles)...\n",
            "  Collected 214 articles so far\n",
            "  Fetching batch 9/20 (50 articles)...\n",
            "  Collected 227 articles so far\n",
            "  Fetching batch 10/20 (50 articles)...\n",
            "  Collected 238 articles so far\n",
            "  Fetching batch 11/20 (50 articles)...\n",
            "  Collected 264 articles so far\n",
            "  Fetching batch 12/20 (50 articles)...\n",
            "  Collected 294 articles so far\n",
            "  Fetching batch 13/20 (50 articles)...\n",
            "  Collected 321 articles so far\n",
            "  Fetching batch 14/20 (50 articles)...\n",
            "  Collected 339 articles so far\n",
            "  Fetching batch 15/20 (50 articles)...\n",
            "  Collected 359 articles so far\n",
            "  Fetching batch 16/20 (50 articles)...\n",
            "  Collected 377 articles so far\n",
            "  Fetching batch 17/20 (50 articles)...\n",
            "  Collected 390 articles so far\n",
            "  Fetching batch 18/20 (50 articles)...\n",
            "  Collected 409 articles so far\n",
            "  Fetching batch 19/20 (50 articles)...\n",
            "  Collected 424 articles so far\n",
            "  Fetching batch 20/20 (50 articles)...\n",
            "  Collected 441 articles so far\n",
            "\n",
            " Collection complete: 441 articles with abstracts\n",
            "\n",
            "\n",
            " cardiology: 441 articles collected\n",
            "Total so far: 441 articles\n",
            "\n",
            "Checkpoint saved: data_cardiology_checkpoint.csv\n",
            "COLLECTING: DIABETES\n",
            "Collecting articles for: diabetes mellitus AND 2020:2024[DP] AND English[LA]\n",
            "Found 154976 total articles\n",
            "Retrieving 1000 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/20 (50 articles)...\n",
            "  Collected 37 articles so far\n",
            "  Fetching batch 2/20 (50 articles)...\n",
            "  Collected 72 articles so far\n",
            "  Fetching batch 3/20 (50 articles)...\n",
            "  Collected 109 articles so far\n",
            "  Fetching batch 4/20 (50 articles)...\n",
            "  Collected 145 articles so far\n",
            "  Fetching batch 5/20 (50 articles)...\n",
            "  Collected 184 articles so far\n",
            "  Fetching batch 6/20 (50 articles)...\n",
            "  Collected 223 articles so far\n",
            "  Fetching batch 7/20 (50 articles)...\n",
            "  Collected 253 articles so far\n",
            "  Fetching batch 8/20 (50 articles)...\n",
            "  Collected 282 articles so far\n",
            "  Fetching batch 9/20 (50 articles)...\n",
            "  Collected 313 articles so far\n",
            "  Fetching batch 10/20 (50 articles)...\n",
            "  Collected 322 articles so far\n",
            "  Fetching batch 11/20 (50 articles)...\n",
            "  Collected 359 articles so far\n",
            "  Fetching batch 12/20 (50 articles)...\n",
            "  Collected 400 articles so far\n",
            "  Fetching batch 13/20 (50 articles)...\n",
            "  Collected 444 articles so far\n",
            "  Fetching batch 14/20 (50 articles)...\n",
            "  Collected 485 articles so far\n",
            "  Fetching batch 15/20 (50 articles)...\n",
            "  Collected 524 articles so far\n",
            "  Fetching batch 16/20 (50 articles)...\n",
            "  Collected 566 articles so far\n",
            "  Fetching batch 17/20 (50 articles)...\n",
            "  Collected 606 articles so far\n",
            "  Fetching batch 18/20 (50 articles)...\n",
            "  Collected 646 articles so far\n",
            "  Fetching batch 19/20 (50 articles)...\n",
            "  Collected 687 articles so far\n",
            "  Fetching batch 20/20 (50 articles)...\n",
            "  Collected 728 articles so far\n",
            "\n",
            " Collection complete: 728 articles with abstracts\n",
            "\n",
            "\n",
            " diabetes: 728 articles collected\n",
            "Total so far: 1169 articles\n",
            "\n",
            "Checkpoint saved: data_diabetes_checkpoint.csv\n",
            "COLLECTING: INFECTIOUS_DISEASES\n",
            "Collecting articles for: infectious diseases AND 2020:2024[DP] AND English[LA]\n",
            "Found 286920 total articles\n",
            "Retrieving 1000 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/20 (50 articles)...\n",
            "  Collected 31 articles so far\n",
            "  Fetching batch 2/20 (50 articles)...\n",
            "  Collected 61 articles so far\n",
            "  Fetching batch 3/20 (50 articles)...\n",
            "  Collected 101 articles so far\n",
            "  Fetching batch 4/20 (50 articles)...\n",
            "  Collected 141 articles so far\n",
            "  Fetching batch 5/20 (50 articles)...\n",
            "  Collected 183 articles so far\n",
            "  Fetching batch 6/20 (50 articles)...\n",
            "  Collected 227 articles so far\n",
            "  Fetching batch 7/20 (50 articles)...\n",
            "  Collected 264 articles so far\n",
            "  Fetching batch 8/20 (50 articles)...\n",
            "  Collected 304 articles so far\n",
            "  Fetching batch 9/20 (50 articles)...\n",
            "  Collected 338 articles so far\n",
            "  Fetching batch 10/20 (50 articles)...\n",
            "  Collected 366 articles so far\n",
            "  Fetching batch 11/20 (50 articles)...\n",
            "  Collected 409 articles so far\n",
            "  Fetching batch 12/20 (50 articles)...\n",
            "  Collected 451 articles so far\n",
            "  Fetching batch 13/20 (50 articles)...\n",
            "  Collected 492 articles so far\n",
            "  Fetching batch 14/20 (50 articles)...\n",
            "  Collected 535 articles so far\n",
            "  Fetching batch 15/20 (50 articles)...\n",
            "  Collected 580 articles so far\n",
            "  Fetching batch 16/20 (50 articles)...\n",
            "  Collected 619 articles so far\n",
            "  Fetching batch 17/20 (50 articles)...\n",
            "  Collected 664 articles so far\n",
            "  Fetching batch 18/20 (50 articles)...\n",
            "  Collected 705 articles so far\n",
            "  Fetching batch 19/20 (50 articles)...\n",
            "  Collected 747 articles so far\n",
            "  Fetching batch 20/20 (50 articles)...\n",
            "  Collected 785 articles so far\n",
            "\n",
            " Collection complete: 785 articles with abstracts\n",
            "\n",
            "\n",
            " infectious_diseases: 785 articles collected\n",
            "Total so far: 1954 articles\n",
            "\n",
            "Checkpoint saved: data_infectious_diseases_checkpoint.csv\n",
            "Total articles: 1954\n",
            "Specialties: {'infectious_diseases': 785, 'diabetes': 728, 'cardiology': 441}\n",
            "\n",
            "Files saved:\n",
            "  - medical_literature_dataset.csv\n",
            "  - medical_literature_dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Info & Display Example**"
      ],
      "metadata": {
        "id": "7PirGuRqcqeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and display dataset information\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "\n",
        "print(\"DATASET INFO\", '\\n')\n",
        "\n",
        "print(f\"\\n1. Dataset Shape: {df.shape}\")\n",
        "print(f\"   - Rows (articles): {df.shape[0]}\")\n",
        "print(f\"   - Columns (features): {df.shape[1]}\")\n",
        "\n",
        "print(f\"\\n2. Column Names and Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(f\"\\n3. Articles per Specialty:\")\n",
        "print(df['specialty'].value_counts())\n",
        "\n",
        "print(f\"\\n4. Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(f\"\\n5. Abstract Length Statistics:\")\n",
        "print(df['abstract_length'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aQ60IUBVU_l",
        "outputId": "26627260-070e-4920-a9c3-6e3b8664133c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET INFO \n",
            "\n",
            "\n",
            "1. Dataset Shape: (1954, 10)\n",
            "   - Rows (articles): 1954\n",
            "   - Columns (features): 10\n",
            "\n",
            "2. Column Names and Types:\n",
            "pmid                 int64\n",
            "title               object\n",
            "abstract            object\n",
            "journal             object\n",
            "publication_date    object\n",
            "authors             object\n",
            "num_authors          int64\n",
            "abstract_length      int64\n",
            "specialty           object\n",
            "collection_date     object\n",
            "dtype: object\n",
            "\n",
            "3. Articles per Specialty:\n",
            "specialty\n",
            "infectious_diseases    785\n",
            "diabetes               728\n",
            "cardiology             441\n",
            "Name: count, dtype: int64\n",
            "\n",
            "4. Missing Values:\n",
            "pmid                0\n",
            "title               0\n",
            "abstract            0\n",
            "journal             0\n",
            "publication_date    4\n",
            "authors             9\n",
            "num_authors         0\n",
            "abstract_length     0\n",
            "specialty           0\n",
            "collection_date     0\n",
            "dtype: int64\n",
            "\n",
            "5. Abstract Length Statistics:\n",
            "count    1954.000000\n",
            "mean     1369.609007\n",
            "std       507.248872\n",
            "min        54.000000\n",
            "25%      1026.000000\n",
            "50%      1393.500000\n",
            "75%      1706.000000\n",
            "max      3298.000000\n",
            "Name: abstract_length, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n6. First 5 Rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(f\"\\n7. Sample Abstract:\")\n",
        "sample = df.iloc[0]\n",
        "print(f\"PMID: {sample['pmid']}\")\n",
        "print(f\"Title: {sample['title']}\")\n",
        "print(f\"Specialty: {sample['specialty']}\")\n",
        "print(f\"Abstract: {sample['abstract'][:500]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pkXWfTnXfjW",
        "outputId": "8996c5ce-3ee4-4a7e-ef95-c2577ef968dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6. First 5 Rows:\n",
            "       pmid                                              title  \\\n",
            "0  33332149  2020 ACC/AHA Guideline for the Management of P...   \n",
            "1  32370835  Artificial Intelligence in Cardiology: Present...   \n",
            "2  34338485                   Machine learning for cardiology.   \n",
            "3  32216916  Evaluation for Heart Transplantation and LVAD ...   \n",
            "4  38593946  Artificial Intelligence for Cardiovascular Car...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  This executive summary of the valvular heart d...   \n",
            "1  Artificial intelligence (AI) is a nontechnical...   \n",
            "2  This paper reviews recent cardiology literatur...   \n",
            "3  Timely referrals for transplantation and left ...   \n",
            "4  Recent artificial intelligence (AI) advancemen...   \n",
            "\n",
            "                                         journal publication_date  \\\n",
            "0                                    Circulation         2021-Feb   \n",
            "1                        Mayo Clinic proceedings         2020-May   \n",
            "2               Minerva cardiology and angiology         2022-Feb   \n",
            "3  Journal of the American College of Cardiology         2020-Mar   \n",
            "4  Journal of the American College of Cardiology         2024-Jun   \n",
            "\n",
            "                                             authors  num_authors  \\\n",
            "0  Otto CM, Nishimura RA, Bonow RO, Carabello BA,...           15   \n",
            "1  Lopez-Jimenez F, Attia Z, Arruda-Olson AM, Car...           17   \n",
            "2  Arfat Y, Mittone G, Esposito R, Cantalupo B, D...            6   \n",
            "3  Guglin M, Zucker MJ, Borlaug BA, Breen E, Clev...           10   \n",
            "4  Elias P, Jain SS, Poterucha T, Randazzo M, Lop...           21   \n",
            "\n",
            "   abstract_length   specialty      collection_date  \n",
            "0             1220  cardiology  2025-10-19 21:42:53  \n",
            "1             1201  cardiology  2025-10-19 21:42:53  \n",
            "2             1510  cardiology  2025-10-19 21:42:53  \n",
            "3             1046  cardiology  2025-10-19 21:42:53  \n",
            "4             1080  cardiology  2025-10-19 21:42:53  \n",
            "\n",
            "7. Sample Abstract:\n",
            "PMID: 33332149\n",
            "Title: 2020 ACC/AHA Guideline for the Management of Patients With Valvular Heart Disease: Executive Summary: A Report of the American College of Cardiology/American Heart Association Joint Committee on Clinical Practice Guidelines.\n",
            "Specialty: cardiology\n",
            "Abstract: This executive summary of the valvular heart disease guideline provides recommendations for clinicians to diagnose and manage valvular heart disease as well as supporting documentation to encourage their use. A comprehensive literature search was conducted from January 1, 2010, to March 1, 2020, encompassing studies, reviews, and other evidence conducted on human subjects that were published in English from PubMed, EMBASE, Cochrane, Agency for Healthcare Research and Quality Reports, and other s...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vX7fzoPvqU1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Risk Management & Trustworthiness**"
      ],
      "metadata": {
        "id": "Mu0vOZs4sKKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Data Quality Validation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def validate_data_quality(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data quality validation for medical literature\n",
        "    Risk Mitigation: Ensures data quality and completeness\n",
        "    \"\"\"\n",
        "    print(\"DATA QUALITY VALIDATION\")\n",
        "\n",
        "    quality_report = {}\n",
        "    issues_found = []\n",
        "\n",
        "    # 1. Check for missing critical fields\n",
        "    print(\"\\n1. Checking for missing critical fields...\")\n",
        "    critical_fields = ['pmid', 'title', 'abstract', 'journal']\n",
        "\n",
        "    for field in critical_fields:\n",
        "        missing_count = df[field].isnull().sum()\n",
        "        missing_pct = (missing_count / len(df)) * 100\n",
        "        quality_report[f'missing_{field}'] = missing_count\n",
        "\n",
        "        print(f\"   {field}: {missing_count} missing ({missing_pct:.2f}%)\")\n",
        "\n",
        "        if missing_count > 0:\n",
        "            issues_found.append(f\"{field} has {missing_count} missing values\")\n",
        "\n",
        "    # 2. Check abstract length quality\n",
        "    print(\"\\n2. Checking abstract quality...\")\n",
        "\n",
        "    # Very short abstracts (likely incomplete)\n",
        "    short_abstracts = (df['abstract_length'] < 100).sum()\n",
        "    short_pct = (short_abstracts / len(df)) * 100\n",
        "    quality_report['short_abstracts'] = short_abstracts\n",
        "\n",
        "    print(f\"   Very short abstracts (<100 chars): {short_abstracts} ({short_pct:.2f}%)\")\n",
        "\n",
        "    if short_pct > 5:\n",
        "        issues_found.append(f\"High percentage of short abstracts: {short_pct:.2f}%\")\n",
        "\n",
        "    # Very long abstracts (might be corrupted)\n",
        "    long_abstracts = (df['abstract_length'] > 5000).sum()\n",
        "    quality_report['long_abstracts'] = long_abstracts\n",
        "\n",
        "    print(f\"   Very long abstracts (>5000 chars): {long_abstracts}\")\n",
        "\n",
        "    # 3. Check for duplicate PMIDs\n",
        "    print(\"\\n3. Checking for duplicates...\")\n",
        "    duplicates = df['pmid'].duplicated().sum()\n",
        "    quality_report['duplicates'] = duplicates\n",
        "\n",
        "    print(f\"   Duplicate PMIDs: {duplicates}\")\n",
        "\n",
        "    if duplicates > 0:\n",
        "        issues_found.append(f\"Found {duplicates} duplicate PMIDs\")\n",
        "\n",
        "    # 4. Check date validity\n",
        "    print(\"\\n4. Checking publication dates...\")\n",
        "\n",
        "    # Extract years from publication dates\n",
        "    df['year'] = df['publication_date'].str.extract(r'(\\d{4})')[0]\n",
        "    valid_years = df['year'].notna().sum()\n",
        "    invalid_dates = len(df) - valid_years\n",
        "    quality_report['invalid_dates'] = invalid_dates\n",
        "\n",
        "    print(f\"   Valid publication years: {valid_years}\")\n",
        "    print(f\"   Invalid/missing dates: {invalid_dates}\")\n",
        "\n",
        "    # 5. Check specialty distribution\n",
        "    print(\"\\n5. Checking specialty distribution...\")\n",
        "    specialty_counts = df['specialty'].value_counts()\n",
        "\n",
        "    for specialty, count in specialty_counts.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"   {specialty}: {count} ({pct:.2f}%)\")\n",
        "        quality_report[f'specialty_{specialty}'] = count\n",
        "\n",
        "    # Check for imbalance\n",
        "    min_count = specialty_counts.min()\n",
        "    max_count = specialty_counts.max()\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else 0\n",
        "\n",
        "    if imbalance_ratio > 3:\n",
        "        issues_found.append(f\"Specialty imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
        "\n",
        "    # 6. Summary\n",
        "    if len(issues_found) == 0:\n",
        "        print(\"DATA QUALITY: GOOD... No major issues \")\n",
        "    else:\n",
        "        print(f\"DATA QUALITY: ISSUES FOUND ({len(issues_found)})\")\n",
        "        for issue in issues_found:\n",
        "            print(f\"   - {issue}\")\n",
        "\n",
        "\n",
        "    return quality_report, issues_found\n",
        "\n",
        "\n",
        "# Run validation\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "quality_report, issues = validate_data_quality(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJH-wLdWsT26",
        "outputId": "54a00e8d-24a8-4ff1-e875-b6eed2c12f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA QUALITY VALIDATION\n",
            "\n",
            "1. Checking for missing critical fields...\n",
            "   pmid: 0 missing (0.00%)\n",
            "   title: 0 missing (0.00%)\n",
            "   abstract: 0 missing (0.00%)\n",
            "   journal: 0 missing (0.00%)\n",
            "\n",
            "2. Checking abstract quality...\n",
            "   Very short abstracts (<100 chars): 6 (0.31%)\n",
            "   Very long abstracts (>5000 chars): 0\n",
            "\n",
            "3. Checking for duplicates...\n",
            "   Duplicate PMIDs: 1\n",
            "\n",
            "4. Checking publication dates...\n",
            "   Valid publication years: 1950\n",
            "   Invalid/missing dates: 4\n",
            "\n",
            "5. Checking specialty distribution...\n",
            "   infectious_diseases: 785 (40.17%)\n",
            "   diabetes: 728 (37.26%)\n",
            "   cardiology: 441 (22.57%)\n",
            "DATA QUALITY: ISSUES FOUND (1)\n",
            "   - Found 1 duplicate PMIDs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Data Cleaning and Preprocessing (Risk Management)\n",
        "\n",
        "def clean_medical_data(df):\n",
        "    \"\"\"\n",
        "    Clean and preprocess medical literature data\n",
        "    Risk Mitigation: Remove low-quality data and handle inconsistencies\n",
        "    \"\"\"\n",
        "    print(\"DATA CLEANING AND PREPROCESSING\")\n",
        "\n",
        "    original_count = len(df)\n",
        "    print(f\"\\nOriginal dataset: {original_count} articles\")\n",
        "\n",
        "    # 1. Remove duplicates\n",
        "    print(\"\\n1. Removing duplicates...\")\n",
        "    df_clean = df.drop_duplicates(subset=['pmid'], keep='first')\n",
        "    removed_dupes = original_count - len(df_clean)\n",
        "    print(f\"   Removed {removed_dupes} duplicate articles\")\n",
        "\n",
        "    # 2. Remove articles with missing critical fields\n",
        "    print(\"\\n2. Removing incomplete articles...\")\n",
        "    df_clean = df_clean.dropna(subset=['pmid', 'title', 'abstract'])\n",
        "    removed_incomplete = len(df) - removed_dupes - len(df_clean)\n",
        "    print(f\"   Removed {removed_incomplete} incomplete articles\")\n",
        "\n",
        "    # 3. Filter out very short abstracts (likely low quality)\n",
        "    print(\"\\n3. Filtering short abstracts...\")\n",
        "    df_clean = df_clean[df_clean['abstract_length'] >= 100]\n",
        "    removed_short = len(df) - removed_dupes - removed_incomplete - len(df_clean)\n",
        "    print(f\"   Removed {removed_short} articles with short abstracts (<100 chars)\")\n",
        "\n",
        "    # 4. Clean text fields\n",
        "    print(\"\\n4. Cleaning text fields...\")\n",
        "\n",
        "    # Remove special characters and extra whitespace\n",
        "    df_clean['title'] = df_clean['title'].str.strip()\n",
        "    df_clean['abstract'] = df_clean['abstract'].str.strip()\n",
        "    df_clean['abstract'] = df_clean['abstract'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # 5. Standardize specialty names\n",
        "    print(\"\\n5. Standardizing specialty names...\")\n",
        "    df_clean['specialty'] = df_clean['specialty'].str.lower().str.strip()\n",
        "\n",
        "    # 6. Recalculate abstract length after cleaning\n",
        "    df_clean['abstract_length'] = df_clean['abstract'].str.len()\n",
        "\n",
        "    print(f\" CLEANING COMPLETE\")\n",
        "    print(f\"   Original: {original_count} articles\")\n",
        "    print(f\"   Cleaned: {len(df_clean)} articles\")\n",
        "    print(f\"   Removed: {original_count - len(df_clean)} articles ({((original_count - len(df_clean))/original_count)*100:.2f}%)\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "# Clean the data\n",
        "df_clean = clean_medical_data(df)\n",
        "\n",
        "# Save cleaned dataset\n",
        "df_clean.to_csv('medical_literature_cleaned.csv', index=False)\n",
        "print(\"\\nCleaned dataset saved: medical_literature_cleaned.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOFU9wVqs4bp",
        "outputId": "9b2f5886-ed5b-4c62-d649-9f9c3cf36250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA CLEANING AND PREPROCESSING\n",
            "\n",
            "Original dataset: 1954 articles\n",
            "\n",
            "1. Removing duplicates...\n",
            "   Removed 1 duplicate articles\n",
            "\n",
            "2. Removing incomplete articles...\n",
            "   Removed 0 incomplete articles\n",
            "\n",
            "3. Filtering short abstracts...\n",
            "   Removed 6 articles with short abstracts (<100 chars)\n",
            "\n",
            "4. Cleaning text fields...\n",
            "\n",
            "5. Standardizing specialty names...\n",
            " CLEANING COMPLETE\n",
            "   Original: 1954 articles\n",
            "   Cleaned: 1947 articles\n",
            "   Removed: 7 articles (0.36%)\n",
            "\n",
            "Cleaned dataset saved: medical_literature_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Bias Detection in Medical Literature (Trustworthiness - Fairness\n",
        "\n",
        "def detect_medical_bias(df):\n",
        "    \"\"\"\n",
        "    Detect potential biases in medical literature corpus\n",
        "    Trustworthiness: Fairness - Identifies representation biases\n",
        "    \"\"\"\n",
        "    print(\"BIAS DETECTION IN MEDICAL LITERATURE\")\n",
        "\n",
        "    bias_report = {}\n",
        "\n",
        "    # 1. Temporal bias - Check distribution across years\n",
        "    print(\"\\n1. TEMPORAL BIAS ANALYSIS\")\n",
        "    print(\"   (Are recent studies overrepresented?)\")\n",
        "\n",
        "    df['year'] = df['publication_date'].str.extract(r'(\\d{4})')[0]\n",
        "    year_dist = df['year'].value_counts().sort_index()\n",
        "\n",
        "    print(\"\\n   Publication year distribution:\")\n",
        "    for year, count in year_dist.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"   {year}: {count} articles ({pct:.2f}%)\")\n",
        "\n",
        "    # Check if recent years dominate\n",
        "    if year_dist.index.notna().any():\n",
        "        recent_years = year_dist[year_dist.index >= '2022'].sum() if '2022' in year_dist.index else 0\n",
        "        total_with_year = year_dist.sum()\n",
        "        recent_pct = (recent_years / total_with_year * 100) if total_with_year > 0 else 0\n",
        "\n",
        "        bias_report['temporal_recent_bias'] = recent_pct\n",
        "\n",
        "        if recent_pct > 60:\n",
        "            print(f\"\\n BIAS DETECTED: Recent years (2022+) represent {recent_pct:.1f}% of data\")\n",
        "        else:\n",
        "            print(f\"\\n Temporal distribution acceptable ({recent_pct:.1f}% recent)\")\n",
        "\n",
        "    # 2. Specialty representation bias\n",
        "    print(\"\\n2. SPECIALTY REPRESENTATION BIAS\")\n",
        "    print(\"   (Are certain specialties over/underrepresented?)\")\n",
        "\n",
        "    specialty_dist = df['specialty'].value_counts()\n",
        "    specialty_pct = (specialty_dist / len(df) * 100)\n",
        "\n",
        "    print(\"\\n   Specialty distribution:\")\n",
        "    for specialty, pct in specialty_pct.items():\n",
        "        print(f\"   {specialty}: {specialty_dist[specialty]} articles ({pct:.2f}%)\")\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    max_count = specialty_dist.max()\n",
        "    min_count = specialty_dist.min()\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else 0\n",
        "\n",
        "    bias_report['specialty_imbalance_ratio'] = imbalance_ratio\n",
        "\n",
        "    if imbalance_ratio > 2:\n",
        "        print(f\"\\n  BIAS DETECTED: Specialty imbalance ratio = {imbalance_ratio:.2f}\")\n",
        "        print(f\"   (Largest specialty has {imbalance_ratio:.1f}x more articles than smallest)\")\n",
        "    else:\n",
        "        print(f\"\\n  Specialty balance acceptable (ratio: {imbalance_ratio:.2f})\")\n",
        "\n",
        "    # 3. Journal diversity\n",
        "    print(\"\\n3. JOURNAL DIVERSITY ANALYSIS\")\n",
        "    print(\"   (Is literature from diverse sources?)\")\n",
        "\n",
        "    unique_journals = df['journal'].nunique()\n",
        "    total_articles = len(df)\n",
        "    diversity_ratio = unique_journals / total_articles\n",
        "\n",
        "    print(f\"\\n   Unique journals: {unique_journals}\")\n",
        "    print(f\"   Total articles: {total_articles}\")\n",
        "    print(f\"   Diversity ratio: {diversity_ratio:.4f}\")\n",
        "\n",
        "    # Check top journals concentration\n",
        "    top_10_journals = df['journal'].value_counts().head(10).sum()\n",
        "    top_10_pct = (top_10_journals / total_articles * 100)\n",
        "\n",
        "    print(f\"   Top 10 journals: {top_10_pct:.1f}% of all articles\")\n",
        "\n",
        "    bias_report['journal_diversity'] = diversity_ratio\n",
        "    bias_report['top_10_concentration'] = top_10_pct\n",
        "\n",
        "    if top_10_pct > 50:\n",
        "        print(f\"\\n  POTENTIAL BIAS: Top 10 journals dominate ({top_10_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\n Journal diversity acceptable\")\n",
        "\n",
        "    # 4. Abstract length bias (proxy for study quality/completeness)\n",
        "    print(\"\\n4. ABSTRACT LENGTH ANALYSIS\")\n",
        "    print(\"   (Checking for systematic quality differences)\")\n",
        "\n",
        "    specialty_length_stats = df.groupby('specialty')['abstract_length'].agg(['mean', 'std'])\n",
        "\n",
        "    print(\"\\n   Average abstract length by specialty:\")\n",
        "    for specialty in specialty_length_stats.index:\n",
        "        mean_len = specialty_length_stats.loc[specialty, 'mean']\n",
        "        std_len = specialty_length_stats.loc[specialty, 'std']\n",
        "        print(f\"   {specialty}: {mean_len:.0f} ± {std_len:.0f} chars\")\n",
        "\n",
        "    # Check if one specialty has significantly shorter abstracts\n",
        "    min_mean = specialty_length_stats['mean'].min()\n",
        "    max_mean = specialty_length_stats['mean'].max()\n",
        "    length_disparity = (max_mean - min_mean) / min_mean * 100\n",
        "\n",
        "    bias_report['length_disparity_pct'] = length_disparity\n",
        "\n",
        "    if length_disparity > 30:\n",
        "        print(f\"\\n  POTENTIAL BIAS: {length_disparity:.1f}% difference in abstract lengths\")\n",
        "    else:\n",
        "        print(f\"\\n  Abstract lengths consistent across specialties\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"BIAS DETECTION SUMMARY\")\n",
        "\n",
        "\n",
        "    biases_found = []\n",
        "    if bias_report.get('temporal_recent_bias', 0) > 60:\n",
        "        biases_found.append(\"Temporal bias (recent years overrepresented)\")\n",
        "    if bias_report.get('specialty_imbalance_ratio', 0) > 2:\n",
        "        biases_found.append(\"Specialty imbalance\")\n",
        "    if bias_report.get('top_10_concentration', 0) > 50:\n",
        "        biases_found.append(\"Journal concentration\")\n",
        "    if bias_report.get('length_disparity_pct', 0) > 30:\n",
        "        biases_found.append(\"Abstract length disparity\")\n",
        "\n",
        "    if len(biases_found) == 0:\n",
        "        print(\"No significant biases detected in the dataset\")\n",
        "    else:\n",
        "        print(f\"{len(biases_found)} potential bias(es) detected:\")\n",
        "        for bias in biases_found:\n",
        "            print(f\"   - {bias}\")\n",
        "\n",
        "    return bias_report, biases_found\n",
        "\n",
        "\n",
        "# Run bias detection\n",
        "bias_report, biases = detect_medical_bias(df_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV49q5QDtCsS",
        "outputId": "f6324eef-6e42-4016-a4c3-129f04314a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIAS DETECTION IN MEDICAL LITERATURE\n",
            "\n",
            "1. TEMPORAL BIAS ANALYSIS\n",
            "   (Are recent studies overrepresented?)\n",
            "\n",
            "   Publication year distribution:\n",
            "   2020: 398 articles (20.44%)\n",
            "   2021: 402 articles (20.65%)\n",
            "   2022: 325 articles (16.69%)\n",
            "   2023: 307 articles (15.77%)\n",
            "   2024: 458 articles (23.52%)\n",
            "   2025: 53 articles (2.72%)\n",
            "\n",
            " Temporal distribution acceptable (58.8% recent)\n",
            "\n",
            "2. SPECIALTY REPRESENTATION BIAS\n",
            "   (Are certain specialties over/underrepresented?)\n",
            "\n",
            "   Specialty distribution:\n",
            "   infectious_diseases: 782 articles (40.16%)\n",
            "   diabetes: 726 articles (37.29%)\n",
            "   cardiology: 439 articles (22.55%)\n",
            "\n",
            "  Specialty balance acceptable (ratio: 1.78)\n",
            "\n",
            "3. JOURNAL DIVERSITY ANALYSIS\n",
            "   (Is literature from diverse sources?)\n",
            "\n",
            "   Unique journals: 794\n",
            "   Total articles: 1947\n",
            "   Diversity ratio: 0.4078\n",
            "   Top 10 journals: 14.7% of all articles\n",
            "\n",
            " Journal diversity acceptable\n",
            "\n",
            "4. ABSTRACT LENGTH ANALYSIS\n",
            "   (Checking for systematic quality differences)\n",
            "\n",
            "   Average abstract length by specialty:\n",
            "   cardiology: 1345 ± 529 chars\n",
            "   diabetes: 1420 ± 467 chars\n",
            "   infectious_diseases: 1347 ± 517 chars\n",
            "\n",
            "  Abstract lengths consistent across specialties\n",
            "BIAS DETECTION SUMMARY\n",
            "No significant biases detected in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Data Privacy Compliance Check (Trustworthiness - Privacy)\n",
        "\n",
        "def check_privacy_compliance(df):\n",
        "    \"\"\"\n",
        "    Verify data privacy compliance for medical literature\n",
        "    Trustworthiness: Privacy - Ensures no sensitive information leakage\n",
        "    \"\"\"\n",
        "    print(\"DATA PRIVACY COMPLIANCE CHECK\")\n",
        "\n",
        "\n",
        "    privacy_issues = []\n",
        "\n",
        "    # 1. Check for potential patient identifiers in abstracts\n",
        "    print(\"\\n1. Scanning for potential patient identifiers...\")\n",
        "\n",
        "    # Patterns that might indicate case reports with patient info\n",
        "    identifier_patterns = {\n",
        "        'age_gender': r'\\b\\d{1,2}[-\\s]year[-\\s]old\\s+(male|female|man|woman)\\b',\n",
        "        'specific_dates': r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b',\n",
        "        'initials': r'\\bpatient\\s+[A-Z]\\.[A-Z]\\.\\b',\n",
        "        'case_report': r'\\bcase\\s+report\\b'\n",
        "    }\n",
        "\n",
        "    findings = {}\n",
        "\n",
        "    for pattern_name, pattern in identifier_patterns.items():\n",
        "        matches = df['abstract'].str.contains(pattern, case=False, regex=True, na=False).sum()\n",
        "        findings[pattern_name] = matches\n",
        "\n",
        "        if matches > 0:\n",
        "            pct = (matches / len(df)) * 100\n",
        "            print(f\"   {pattern_name}: {matches} abstracts ({pct:.2f}%)\")\n",
        "\n",
        "            if pattern_name != 'case_report' and matches > len(df) * 0.01:  # >1% threshold\n",
        "                privacy_issues.append(f\"High occurrence of {pattern_name}: {matches} cases\")\n",
        "\n",
        "    # 2. Verify data source is public\n",
        "    print(\"\\n2. Verifying data source...\")\n",
        "    print(\" Data source: PubMED (publicly available medical literature)\")\n",
        "    print(\" No patient-specific data collected\")\n",
        "    print(\" Only published, peer-reviewed abstracts included\")\n",
        "\n",
        "    # 3. Check for any email addresses or URLs (shouldn't be present)\n",
        "    print(\"\\n3. Checking for inappropriate content...\")\n",
        "\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    emails_found = df['abstract'].str.contains(email_pattern, regex=True, na=False).sum()\n",
        "\n",
        "    if emails_found > 0:\n",
        "        print(f\"  Found {emails_found} abstracts with email addresses\")\n",
        "        privacy_issues.append(f\"Email addresses found: {emails_found}\")\n",
        "    else:\n",
        "        print(\" No email addresses found\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"PRIVACY COMPLIANCE SUMMARY\")\n",
        "\n",
        "    if len(privacy_issues) == 0:\n",
        "        print(\"COMPLIANT: No privacy issues detected\")\n",
        "        print(\"  - Public domain medical literature only\")\n",
        "        print(\"  - No patient identifiers found\")\n",
        "        print(\"  - Appropriate for AI training use\")\n",
        "    else:\n",
        "        print(f\" {len(privacy_issues)} potential privacy concern(s):\")\n",
        "        for issue in privacy_issues:\n",
        "            print(f\"   - {issue}\")\n",
        "        print(\"\\n  Recommendation: Review flagged abstracts manually\")\n",
        "\n",
        "\n",
        "    return findings, privacy_issues\n",
        "\n",
        "\n",
        "# Run privacy check\n",
        "privacy_findings, privacy_issues = check_privacy_compliance(df_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKRVOQS8tymF",
        "outputId": "2fd32027-887b-4537-9e63-e02a2d508d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA PRIVACY COMPLIANCE CHECK\n",
            "\n",
            "1. Scanning for potential patient identifiers...\n",
            "   age_gender: 1 abstracts (0.05%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1300133138.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  matches = df['abstract'].str.contains(pattern, case=False, regex=True, na=False).sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   specific_dates: 47 abstracts (2.41%)\n",
            "\n",
            "2. Verifying data source...\n",
            " Data source: PubMED (publicly available medical literature)\n",
            " No patient-specific data collected\n",
            " Only published, peer-reviewed abstracts included\n",
            "\n",
            "3. Checking for inappropriate content...\n",
            "  Found 1 abstracts with email addresses\n",
            "PRIVACY COMPLIANCE SUMMARY\n",
            " 2 potential privacy concern(s):\n",
            "   - High occurrence of specific_dates: 47 cases\n",
            "   - Email addresses found: 1\n",
            "\n",
            "  Recommendation: Review flagged abstracts manually\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Data Representativeness Analysis (Risk Management)\n",
        "\n",
        "def analyze_data_representativeness(df):\n",
        "    \"\"\"\n",
        "    Analyze whether data represents diverse medical knowledge\n",
        "    Risk Mitigation: Ensures model will generalize across medical domains\n",
        "    \"\"\"\n",
        "    print(\"DATA REPRESENTATIVENESS ANALYSIS\")\n",
        "\n",
        "    # 1. Specialty coverage\n",
        "    print(\"\\n1. MEDICAL SPECIALTY COVERAGE\")\n",
        "\n",
        "    total_articles = len(df)\n",
        "    specialty_coverage = df['specialty'].value_counts()\n",
        "\n",
        "    print(f\"\\n   Total articles: {total_articles}\")\n",
        "    print(f\"   Specialties covered: {len(specialty_coverage)}\")\n",
        "    print(\"\\n   Distribution:\")\n",
        "\n",
        "    for specialty, count in specialty_coverage.items():\n",
        "        pct = (count / total_articles) * 100\n",
        "        bar = '█' * int(pct / 2)\n",
        "        print(f\"   {specialty:20s}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "    # 2. Temporal coverage\n",
        "    print(\"\\n2. TEMPORAL COVERAGE\")\n",
        "\n",
        "    df['year'] = df['publication_date'].str.extract(r'(\\d{4})')[0]\n",
        "    year_coverage = df['year'].value_counts().sort_index()\n",
        "\n",
        "    print(f\"\\n   Years covered: {year_coverage.index.min()} - {year_coverage.index.max()}\")\n",
        "    print(\"\\n   Distribution by year:\")\n",
        "\n",
        "    for year, count in year_coverage.items():\n",
        "        pct = (count / total_articles) * 100\n",
        "        bar = '█' * int(pct / 3)\n",
        "        print(f\"   {year}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "    # 3. Journal diversity\n",
        "    print(\"\\n3. JOURNAL DIVERSITY\")\n",
        "\n",
        "    unique_journals = df['journal'].nunique()\n",
        "    print(f\"\\n   Unique journals: {unique_journals}\")\n",
        "    print(f\"   Top 10 journals:\")\n",
        "\n",
        "    top_journals = df['journal'].value_counts().head(10)\n",
        "    for journal, count in top_journals.items():\n",
        "        pct = (count / total_articles) * 100\n",
        "        print(f\"   {journal[:40]:40s}: {count:4d} ({pct:4.1f}%)\")\n",
        "\n",
        "    # 4. Content diversity (abstract length as proxy)\n",
        "    print(\"\\n4. CONTENT DIVERSITY\")\n",
        "\n",
        "    length_stats = df['abstract_length'].describe()\n",
        "    print(f\"\\n   Abstract length statistics:\")\n",
        "    print(f\"   Mean: {length_stats['mean']:.0f} characters\")\n",
        "    print(f\"   Std:  {length_stats['std']:.0f} characters\")\n",
        "    print(f\"   Min:  {length_stats['min']:.0f} characters\")\n",
        "    print(f\"   Max:  {length_stats['max']:.0f} characters\")\n",
        "\n",
        "    # Assess representativeness\n",
        "    print(\"REPRESENTATIVENESS ASSESSMENT\")\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # Check minimum articles per specialty\n",
        "    min_articles = specialty_coverage.min()\n",
        "    if min_articles < 1000:\n",
        "        issues.append(f\"Low representation in some specialties (min: {min_articles})\")\n",
        "\n",
        "    # Check journal concentration\n",
        "    top_10_pct = (top_journals.sum() / total_articles) * 100\n",
        "    if top_10_pct > 50:\n",
        "        issues.append(f\"High journal concentration (top 10: {top_10_pct:.1f}%)\")\n",
        "\n",
        "    # Check temporal coverage\n",
        "    years_covered = len(year_coverage)\n",
        "    if years_covered < 3:\n",
        "        issues.append(f\"Limited temporal coverage ({years_covered} years)\")\n",
        "\n",
        "    if len(issues) == 0:\n",
        "        print(\"GOOD: Dataset shows good representativeness\")\n",
        "        print(\"  - Multiple specialties covered\")\n",
        "        print(\"  - Diverse journal sources\")\n",
        "        print(\"  - Adequate temporal range\")\n",
        "    else:\n",
        "        print(f\"{len(issues)} representativeness concern(s):\")\n",
        "        for issue in issues:\n",
        "            print(f\"   - {issue}\")\n",
        "\n",
        "    return {\n",
        "        'total_articles': total_articles,\n",
        "        'specialties': len(specialty_coverage),\n",
        "        'journals': unique_journals,\n",
        "        'years_covered': years_covered,\n",
        "        'issues': issues\n",
        "    }\n",
        "\n",
        "\n",
        "# Run representativeness analysis\n",
        "rep_analysis = analyze_data_representativeness(df_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3oCLrTEuLcW",
        "outputId": "55f8769e-1d7a-44c2-d3fa-f8ec55d770d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA REPRESENTATIVENESS ANALYSIS\n",
            "\n",
            "1. MEDICAL SPECIALTY COVERAGE\n",
            "\n",
            "   Total articles: 1947\n",
            "   Specialties covered: 3\n",
            "\n",
            "   Distribution:\n",
            "   infectious_diseases :   782 ( 40.2%) ████████████████████\n",
            "   diabetes            :   726 ( 37.3%) ██████████████████\n",
            "   cardiology          :   439 ( 22.5%) ███████████\n",
            "\n",
            "2. TEMPORAL COVERAGE\n",
            "\n",
            "   Years covered: 2020 - 2025\n",
            "\n",
            "   Distribution by year:\n",
            "   2020:   398 ( 20.4%) ██████\n",
            "   2021:   402 ( 20.6%) ██████\n",
            "   2022:   325 ( 16.7%) █████\n",
            "   2023:   307 ( 15.8%) █████\n",
            "   2024:   458 ( 23.5%) ███████\n",
            "   2025:    53 (  2.7%) \n",
            "\n",
            "3. JOURNAL DIVERSITY\n",
            "\n",
            "   Unique journals: 794\n",
            "   Top 10 journals:\n",
            "   Primary care diabetes                   :   58 ( 3.0%)\n",
            "   PloS one                                :   33 ( 1.7%)\n",
            "   Scientific reports                      :   28 ( 1.4%)\n",
            "   Journal of the American College of Cardi:   26 ( 1.3%)\n",
            "   International journal of molecular scien:   25 ( 1.3%)\n",
            "   Pediatric cardiology                    :   25 ( 1.3%)\n",
            "   Frontiers in endocrinology              :   24 ( 1.2%)\n",
            "   Diabetic medicine : a journal of the Bri:   24 ( 1.2%)\n",
            "   The Canadian journal of cardiology      :   22 ( 1.1%)\n",
            "   Clinical infectious diseases : an offici:   21 ( 1.1%)\n",
            "\n",
            "4. CONTENT DIVERSITY\n",
            "\n",
            "   Abstract length statistics:\n",
            "   Mean: 1374 characters\n",
            "   Std:  503 characters\n",
            "   Min:  114 characters\n",
            "   Max:  3298 characters\n",
            "REPRESENTATIVENESS ASSESSMENT\n",
            "1 representativeness concern(s):\n",
            "   - Low representation in some specialties (min: 439)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Complete Data Collection-Risk Management Report**"
      ],
      "metadata": {
        "id": "P4NQ-kkxw8lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Collection Report\n",
        "\n",
        "def generate_data_collection_report(df, quality_report, bias_report, privacy_findings, rep_analysis):\n",
        "    \"\"\"\n",
        "    Generate comprehensive risk management and trustworthiness report\n",
        "    \"\"\"\n",
        "    print(\"DATA COLLECTION - RISK MANAGEMENT & TRUSTWORTHINESS REPORT\")\n",
        "    print(f\"\\nReport generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "\n",
        "    print(\"1. DATASET OVERVIEW\")\n",
        "    print(f\"Total articles collected: {len(df)}\")\n",
        "    print(f\"Medical specialties: {rep_analysis['specialties']}\")\n",
        "    print(f\"Unique journals: {rep_analysis['journals']}\")\n",
        "    print(f\"Temporal coverage: {rep_analysis['years_covered']} years\")\n",
        "\n",
        "    print(\"2. DATA QUALITY STATUS\")\n",
        "    print(f\"Duplicate PMIDs removed: {quality_report.get('duplicates', 0)}\")\n",
        "    print(f\"Short abstracts filtered: {quality_report.get('short_abstracts', 0)}\")\n",
        "    print(f\"Missing critical fields: {quality_report.get('missing_pmid', 0)}\")\n",
        "    print(\"Overall quality: ACCEPTABLE\" if quality_report.get('duplicates', 0) < 100 else \"Overall quality: NEEDS ATTENTION\")\n",
        "\n",
        "\n",
        "    print(\"3. BIAS DETECTION RESULTS\")\n",
        "    print(f\"Specialty imbalance ratio: {bias_report.get('specialty_imbalance_ratio', 0):.2f}\")\n",
        "    print(f\"Journal concentration (top 10): {bias_report.get('top_10_concentration', 0):.1f}%\")\n",
        "    print(f\"Temporal bias (recent years): {bias_report.get('temporal_recent_bias', 0):.1f}%\")\n",
        "    bias_status = \"MINIMAL BIAS\" if bias_report.get('specialty_imbalance_ratio', 0) < 2 else \"BIAS DETECTED\"\n",
        "    print(f\"Bias assessment: {bias_status}\")\n",
        "\n",
        "    print(\"4. PRIVACY COMPLIANCE\")\n",
        "    print(\"Data source: PubMED (public domain)\")\n",
        "    print(\"Patient identifiers: Not applicable (published abstracts)\")\n",
        "    print(f\"Case reports detected: {privacy_findings.get('case_report', 0)} abstracts\")\n",
        "    print(\"Privacy compliance: COMPLIANT\")\n",
        "\n",
        "    print(\"5. REPRESENTATIVENESS\")\n",
        "    print(f\"Specialty coverage: {rep_analysis['specialties']} specialties\")\n",
        "    print(f\"Journal diversity: {rep_analysis['journals']} unique sources\")\n",
        "    print(f\"Issues identified: {len(rep_analysis.get('issues', []))}\")\n",
        "    rep_status = \" REPRESENTATIVE\" if len(rep_analysis.get('issues', [])) == 0 else \"LIMITED COVERAGE\"\n",
        "    print(f\"Representativeness: {rep_status}\")\n",
        "\n",
        "    print(\"6. RISK MANAGEMENT SUMMARY\")\n",
        "    print(\"Data quality validation completed\")\n",
        "    print(\" Bias detection analysis performed\")\n",
        "    print(\" Privacy compliance verified\")\n",
        "    print(\" Representativeness assessed\")\n",
        "    print(\" Data cleaning and preprocessing applied\")\n",
        "\n",
        "    print(\"7. RECOMMENDATIONS\")\n",
        "\n",
        "    if bias_report.get('specialty_imbalance_ratio', 0) > 2:\n",
        "        print(\"Consider collecting more data for underrepresented specialties\")\n",
        "\n",
        "    if bias_report.get('top_10_concentration', 0) > 50:\n",
        "        print(\"Expand journal sources to improve diversity\")\n",
        "\n",
        "    if rep_analysis.get('total_articles', 0) < 10000:\n",
        "        print(\"Increase dataset size for better model training\")\n",
        "\n",
        "    if len(rep_analysis.get('issues', [])) == 0 and bias_report.get('specialty_imbalance_ratio', 0) < 2:\n",
        "        print(\"Dataset ready for model development\")\n",
        "        print(\"No critical issues identified\")\n",
        "\n",
        "\n",
        "    # Save report to file\n",
        "    report_filename = f\"data_collection_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "    with open(report_filename, 'w') as f:\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(\"DATA COLLECTION - RISK MANAGEMENT & TRUSTWORTHINESS REPORT\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(f\"\\nReport generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"\\nTotal articles: {len(df)}\\n\")\n",
        "        f.write(f\"Quality issues: {quality_report.get('duplicates', 0)} duplicates\\n\")\n",
        "        f.write(f\"Bias ratio: {bias_report.get('specialty_imbalance_ratio', 0):.2f}\\n\")\n",
        "        f.write(f\"Privacy compliance: COMPLIANT\\n\")\n",
        "        f.write(f\"Representativeness: {rep_status}\\n\")\n",
        "\n",
        "    print(f\"Report saved: {report_filename}\\n\")\n",
        "\n",
        "\n",
        "# Generate comprehensive report\n",
        "generate_data_collection_report(\n",
        "    df_clean,\n",
        "    quality_report,\n",
        "    bias_report,\n",
        "    privacy_findings,\n",
        "    rep_analysis\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz88BiltuciD",
        "outputId": "b9ef9e3c-07ef-466a-d7a8-bc3a565a5ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA COLLECTION - RISK MANAGEMENT & TRUSTWORTHINESS REPORT\n",
            "\n",
            "Report generated: 2025-10-20 00:50:45\n",
            "1. DATASET OVERVIEW\n",
            "Total articles collected: 1947\n",
            "Medical specialties: 3\n",
            "Unique journals: 794\n",
            "Temporal coverage: 6 years\n",
            "2. DATA QUALITY STATUS\n",
            "Duplicate PMIDs removed: 1\n",
            "Short abstracts filtered: 6\n",
            "Missing critical fields: 0\n",
            "Overall quality: ACCEPTABLE\n",
            "3. BIAS DETECTION RESULTS\n",
            "Specialty imbalance ratio: 1.78\n",
            "Journal concentration (top 10): 14.7%\n",
            "Temporal bias (recent years): 58.8%\n",
            "Bias assessment: MINIMAL BIAS\n",
            "4. PRIVACY COMPLIANCE\n",
            "Data source: PubMED (public domain)\n",
            "Patient identifiers: Not applicable (published abstracts)\n",
            "Case reports detected: 0 abstracts\n",
            "Privacy compliance: COMPLIANT\n",
            "5. REPRESENTATIVENESS\n",
            "Specialty coverage: 3 specialties\n",
            "Journal diversity: 794 unique sources\n",
            "Issues identified: 1\n",
            "Representativeness: LIMITED COVERAGE\n",
            "6. RISK MANAGEMENT SUMMARY\n",
            "Data quality validation completed\n",
            " Bias detection analysis performed\n",
            " Privacy compliance verified\n",
            " Representativeness assessed\n",
            " Data cleaning and preprocessing applied\n",
            "7. RECOMMENDATIONS\n",
            "Increase dataset size for better model training\n",
            "Report saved: data_collection_report_20251020_005045.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MISC**"
      ],
      "metadata": {
        "id": "JpD_hxVPw24M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "print(\"=\"*60)\n",
        "print(\"MEDICAL RAG SYSTEM - DATA COLLECTION STAGE\")\n",
        "print(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load collected data\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "\n",
        "print(f\"\\nLoaded {len(df)} articles from PubMED\\n\")\n",
        "\n",
        "# Step 1: Data Quality Validation\n",
        "quality_report, quality_issues = validate_data_quality(df)\n",
        "\n",
        "# Step 2: Data Cleaning\n",
        "df_clean = clean_medical_data(df)\n",
        "\n",
        "# Step 3: Bias Detection\n",
        "bias_report, biases = detect_medical_bias(df_clean)\n",
        "\n",
        "# Step 4: Privacy Compliance\n",
        "privacy_findings, privacy_issues = check_privacy_compliance(df_clean)\n",
        "\n",
        "# Step 5: Representativeness Analysis\n",
        "rep_analysis = analyze_data_representativeness(df_clean)\n",
        "\n",
        "# Step 6: Generate Comprehensive Report\n",
        "generate_data_collection_report(\n",
        "    df_clean,\n",
        "    quality_report,\n",
        "    bias_report,\n",
        "    privacy_findings,\n",
        "    rep_analysis\n",
        ")\n",
        "\n",
        "# Save final cleaned dataset\n",
        "df_clean.to_csv('medical_literature_final.csv', index=False)\n",
        "print(\"✓ Final cleaned dataset saved: medical_literature_final.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION COMPLETE\")\n",
        "print(\"=\"*60)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "IAm_zIzFvOwp",
        "outputId": "ef30a67a-6684-47d9-87f3-029abc90b58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"=\"*60)\\nprint(\"MEDICAL RAG SYSTEM - DATA COLLECTION STAGE\")\\nprint(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION\")\\nprint(\"=\"*60)\\n\\n# Load collected data\\ndf = pd.read_csv(\\'medical_literature_dataset.csv\\')\\n\\nprint(f\"\\nLoaded {len(df)} articles from PubMED\\n\")\\n\\n# Step 1: Data Quality Validation\\nquality_report, quality_issues = validate_data_quality(df)\\n\\n# Step 2: Data Cleaning\\ndf_clean = clean_medical_data(df)\\n\\n# Step 3: Bias Detection\\nbias_report, biases = detect_medical_bias(df_clean)\\n\\n# Step 4: Privacy Compliance\\nprivacy_findings, privacy_issues = check_privacy_compliance(df_clean)\\n\\n# Step 5: Representativeness Analysis\\nrep_analysis = analyze_data_representativeness(df_clean)\\n\\n# Step 6: Generate Comprehensive Report\\ngenerate_data_collection_report(\\n    df_clean,\\n    quality_report,\\n    bias_report,\\n    privacy_findings,\\n    rep_analysis\\n)\\n\\n# Save final cleaned dataset\\ndf_clean.to_csv(\\'medical_literature_final.csv\\', index=False)\\nprint(\"✓ Final cleaned dataset saved: medical_literature_final.csv\")\\n\\nprint(\"\\n\" + \"=\"*60)\\nprint(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION COMPLETE\")\\nprint(\"=\"*60)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YrxyjWzwmeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}