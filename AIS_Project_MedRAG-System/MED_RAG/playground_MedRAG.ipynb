{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io6Xw6UbXqV8"
      },
      "source": [
        "## **Installing Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vlBEHOO7VWFR",
        "outputId": "0f7e957b-883d-474f-e5cf-5858b328efa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.86)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.2.6)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.32.5)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.8.7-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
            "Collecting scispacy\n",
            "  Using cached scispacy-0.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Using cached murmurhash-1.0.13-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Using cached cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Using cached preshed-3.0.10-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Using cached thinc-8.3.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Using cached srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Using cached typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (2.2.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from spacy) (25.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Using cached blis-1.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached smart_open-7.4.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached wrapt-2.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scispacy) (1.15.3)\n",
            "Collecting conllu (from scispacy)\n",
            "  Using cached conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "INFO: pip is looking at multiple versions of scispacy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scispacy\n",
            "  Using cached scispacy-0.5.5-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.7.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scispacy) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scispacy) (1.7.2)\n",
            "Collecting pysbd (from scispacy)\n",
            "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting nmslib>=1.7.3.6 (from scispacy)\n",
            "  Using cached nmslib-2.1.2.tar.gz (197 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Downloading thinc-8.2.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting numpy<3.0.0,>=1.19.0 (from blis<1.4.0,>=1.3.0->thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.3.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
            "Collecting pybind11>=2.2.3 (from nmslib>=1.7.3.6->scispacy)\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from jinja2->spacy) (3.0.3)\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
            "Downloading preshed-3.0.10-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (634 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.4/634.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
            "Downloading smart_open-7.4.2-py3-none-any.whl (63 kB)\n",
            "Downloading scispacy-0.5.5-py3-none-any.whl (46 kB)\n",
            "Downloading spacy-3.7.5-cp310-cp310-macosx_11_0_arm64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.5-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.0/779.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading marisa_trie-1.3.1-cp310-cp310-macosx_11_0_arm64.whl (156 kB)\n",
            "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "Downloading wrapt-2.0.0-cp310-cp310-macosx_11_0_arm64.whl (61 kB)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for nmslib: filename=nmslib-2.1.2-cp310-cp310-macosx_11_0_arm64.whl size=730986 sha256=df30555998ba8df72abe0cae7242578888e3ef554f9e5a9bf7cd037d5930dc09\n",
            "  Stored in directory: /Users/meghasuhanth/Library/Caches/pip/wheels/69/ea/3e/d97c2d4d55457d6e4c87f66f96fd37f5bd64c1f4f0df71bebf\n",
            "Successfully built nmslib\n",
            "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pysbd, pybind11, numpy, murmurhash, mdurl, marisa-trie, conllu, cloudpathlib, catalogue, srsly, smart-open, preshed, markdown-it-py, language-data, blis, rich, nmslib, langcodes, confection, typer, thinc, weasel, spacy, scispacy\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.2.6\n",
            "\u001b[2K    Uninstalling numpy-2.2.6:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [scispacy]/30\u001b[0m [spacy]ge-data]\n",
            "\u001b[1A\u001b[2KSuccessfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 conllu-6.0.0 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 nmslib-2.1.2 numpy-1.26.4 preshed-3.0.10 pybind11-3.0.1 pysbd-0.3.4 rich-14.2.0 scispacy-0.5.5 shellingham-1.5.4 smart-open-7.4.2 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.2.5 typer-0.20.0 wasabi-1.1.3 weasel-0.4.1 wrapt-2.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install biopython pandas numpy requests python-dotenv\n",
        "\n",
        "# For medical NLP\n",
        "!pip install spacy scispacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44NTY85iU_e2",
        "outputId": "7f839abb-561e-4977-9441-f0698a75fd52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (4.57.1)\n",
            "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (5.1.2)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.9.0)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.7.2)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "#for vector embedding generation\n",
        "!pip install transformers sentence-transformers torch pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-PZbtLlkf42",
        "outputId": "637f5940-e447-4c61-a06a-60ced1f4ebb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.3.3)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.7.2)\n",
            "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (5.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (2.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "  #for vector database setup\n",
        "\n",
        "# Install FAISS and dependencies\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Other utilities\n",
        "!pip install numpy pandas scikit-learn sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8DKCuia-y4E",
        "outputId": "e74a3e8d-0bf9-4d80-98e4-9d7452291a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.6.1)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.72.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Collecting docstring-parser<1,>=0.15 (from anthropic)\n",
            "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading anthropic-0.72.0-py3-none-any.whl (357 kB)\n",
            "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: docstring-parser, anthropic\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [anthropic]\n",
            "\u001b[1A\u001b[2KSuccessfully installed anthropic-0.72.0 docstring-parser-0.17.0\n",
            "Requirement already satisfied: transformers in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests->transformers) (2025.10.5)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain)\n",
            "  Downloading langchain_core-1.0.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain) (2.12.3)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
            "  Downloading langsmith-0.4.39-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.11.0-cp310-cp310-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading orjson-3.11.4-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
            "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
            "  Downloading zstandard-0.25.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain-openai) (2.6.1)\n",
            "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n",
            "  Downloading tiktoken-0.12.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.10.23)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.69.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from langchain-anthropic) (0.72.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
            "Downloading langchain-1.0.3-py3-none-any.whl (91 kB)\n",
            "Downloading langchain_core-1.0.2-py3-none-any.whl (469 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langgraph-1.0.2-py3-none-any.whl (156 kB)\n",
            "Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
            "Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "Downloading langsmith-0.4.39-py3-none-any.whl (397 kB)\n",
            "Downloading langchain_openai-1.0.1-py3-none-any.whl (81 kB)\n",
            "Downloading tiktoken-0.12.0-cp310-cp310-macosx_11_0_arm64.whl (995 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m995.8/995.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_anthropic-1.0.1-py3-none-any.whl (46 kB)\n",
            "Downloading orjson-3.11.4-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (243 kB)\n",
            "Downloading ormsgpack-1.11.0-cp310-cp310-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (367 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading zstandard-0.25.0-cp310-cp310-macosx_11_0_arm64.whl (640 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.6/640.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard, xxhash, ormsgpack, orjson, jsonpatch, tiktoken, requests-toolbelt, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langchain-anthropic, langgraph-prebuilt, langgraph, langchain\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [langchain]16\u001b[0m [langgraph]core]\n",
            "\u001b[1A\u001b[2KSuccessfully installed jsonpatch-1.33 langchain-1.0.3 langchain-anthropic-1.0.1 langchain-core-1.0.2 langchain-openai-1.0.1 langgraph-1.0.2 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 langsmith-0.4.39 orjson-3.11.4 ormsgpack-1.11.0 requests-toolbelt-1.0.0 tiktoken-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (1.2.1)\n",
            "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from tiktoken) (2025.10.23)\n",
            "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Install LLM libraries for RAG pipeline\n",
        "!pip install openai anthropic  # For API-based LLMs\n",
        "# OR\n",
        "!pip install transformers torch  # For local open-source LLMs\n",
        "\n",
        "# Install RAG framework\n",
        "!pip install langchain langchain-openai langchain-anthropic\n",
        "\n",
        "# Utilities\n",
        "!pip install python-dotenv tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting google-api-core (from google-generativeai)\n",
            "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting google-api-python-client (from google-generativeai)\n",
            "  Downloading google_api_python_client-2.186.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
            "  Downloading google_auth-2.42.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from google-generativeai) (6.33.0)\n",
            "Requirement already satisfied: pydantic in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from google-generativeai) (4.15.0)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
            "  Downloading googleapis_common_protos-1.71.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio-1.76.0-cp310-cp310-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.1)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
            "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
            "  Downloading google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
            "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pyparsing<4,>=3.0.4 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/medrag/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m245.6 kB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
            "Downloading google_auth-2.42.1-py2.py3-none-any.whl (222 kB)\n",
            "Downloading googleapis_common_protos-1.71.0-py3-none-any.whl (294 kB)\n",
            "Downloading grpcio-1.76.0-cp310-cp310-macosx_11_0_universal2.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m313.9 kB/s\u001b[0m  \u001b[33m0:00:36\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
            "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading google_api_python_client-2.186.0-py3-none-any.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m310.0 kB/s\u001b[0m  \u001b[33m0:00:48\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
            "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: uritemplate, pyparsing, pyasn1, protobuf, grpcio, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
            "\u001b[2K  Attempting uninstall: protobuf\n",
            "\u001b[2K    Found existing installation: protobuf 6.33.0\n",
            "\u001b[2K    Uninstalling protobuf-6.33.0:\n",
            "\u001b[2K      Successfully uninstalled protobuf-6.33.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [google-generativeai]ogle-ai-generativelanguage]\n",
            "\u001b[1A\u001b[2KSuccessfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.28.1 google-api-python-client-2.186.0 google-auth-2.42.1 google-auth-httplib2-0.2.1 google-generativeai-0.8.5 googleapis-common-protos-1.71.0 grpcio-1.76.0 grpcio-status-1.71.2 httplib2-0.31.0 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.2.5 rsa-4.9.1 uritemplate-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkpveqXqYu_E"
      },
      "source": [
        "## **Collecting Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TgyuMN04VDpe"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "Entrez.email = \"meghsuhanths2306@gmail.com\"\n",
        "\n",
        "def collect_pubmed_fixed(search_term, max_results=100):\n",
        "    \"\"\"\n",
        "    Fixed PubMED collection with proper XML handling\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Collecting articles for: {search_term}\")\n",
        "\n",
        "\n",
        "    #Searching for PMIDs\n",
        "    try:\n",
        "        handle = Entrez.esearch(\n",
        "            db=\"pubmed\",\n",
        "            term=search_term,\n",
        "            retmax=max_results,\n",
        "            sort=\"relevance\"\n",
        "        )\n",
        "\n",
        "        results = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        pmids = results['IdList']\n",
        "        total_count = int(results['Count'])\n",
        "\n",
        "        print(f\"Found {total_count} total articles\")\n",
        "        print(f\"Retrieving {len(pmids)} articles\\n\")\n",
        "\n",
        "        if len(pmids) == 0:\n",
        "            print(\"No PMIDs found!\")\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "    #Fetching abstracts\n",
        "    print(\"Step 2: Fetching article details...\")\n",
        "    articles = []\n",
        "    batch_size = 50\n",
        "\n",
        "    for i in range(0, len(pmids), batch_size):\n",
        "        batch_pmids = pmids[i:i+batch_size]\n",
        "        batch_num = (i // batch_size) + 1\n",
        "        total_batches = (len(pmids) + batch_size - 1) // batch_size\n",
        "\n",
        "        print(f\"  Fetching batch {batch_num}/{total_batches} ({len(batch_pmids)} articles)...\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            handle = Entrez.efetch(\n",
        "                db=\"pubmed\",\n",
        "                id=batch_pmids,\n",
        "                rettype=\"abstract\",\n",
        "                retmode=\"xml\"\n",
        "            )\n",
        "            records = Entrez.read(handle)\n",
        "            handle.close()\n",
        "\n",
        "            # Processing each article\n",
        "            for record in records['PubmedArticle']:\n",
        "                try:\n",
        "                    medline_citation = record['MedlineCitation']\n",
        "                    article = medline_citation['Article']\n",
        "\n",
        "                    # Extracting PMID, title, abstract, journal, publication date, author\n",
        "                    pmid = str(medline_citation['PMID'])\n",
        "                    title = article.get('ArticleTitle', '')\n",
        "\n",
        "                    abstract = ''\n",
        "                    if 'Abstract' in article:\n",
        "                        abstract_texts = article['Abstract'].get('AbstractText', [])\n",
        "                        # Joining all abstract parts\n",
        "                        abstract = ' '.join([str(text) for text in abstract_texts])\n",
        "\n",
        "                    journal = article.get('Journal', {}).get('Title', '')\n",
        "\n",
        "                    # Extracting publication date\n",
        "                    pub_date = ''\n",
        "                    if 'Journal' in article and 'JournalIssue' in article['Journal']:\n",
        "                        pub_info = article['Journal']['JournalIssue'].get('PubDate', {})\n",
        "                        year = pub_info.get('Year', '')\n",
        "                        month = pub_info.get('Month', '')\n",
        "                        pub_date = f\"{year}-{month}\" if year else ''\n",
        "\n",
        "                    authors = []\n",
        "                    if 'AuthorList' in article:\n",
        "                        for author in article['AuthorList']:\n",
        "                            if 'LastName' in author:\n",
        "                                name = f\"{author.get('LastName', '')} {author.get('Initials', '')}\"\n",
        "                                authors.append(name.strip())\n",
        "\n",
        "                    # Only keep if we have an abstract\n",
        "                    if abstract and len(abstract) > 50:\n",
        "                        article_data = {\n",
        "                            'pmid': pmid,\n",
        "                            'title': title,\n",
        "                            'abstract': abstract,\n",
        "                            'journal': journal,\n",
        "                            'publication_date': pub_date,\n",
        "                            'authors': ', '.join(authors),\n",
        "                            'num_authors': len(authors),\n",
        "                            'abstract_length': len(abstract)\n",
        "                        }\n",
        "                        articles.append(article_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Warning: Skipped one article: {str(e)[:50]}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"  Collected {len(articles)} articles so far\")\n",
        "            time.sleep(0.4)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Batch {batch_num} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n Collection complete: {len(articles)} articles with abstracts\\n\")\n",
        "    return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4yrjtrxZtB2",
        "outputId": "362cc401-1edb-49ac-b4ac-1ffd7250e63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING DATA COLLECTION\n",
            "Collecting articles for: diabetes\n",
            "Found 1081017 total articles\n",
            "Retrieving 50 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/1 (50 articles)...\n",
            "  Collected 38 articles so far\n",
            "\n",
            " Collection complete: 38 articles with abstracts\n",
            "\n",
            "Collected 38 articles\n",
            "\n",
            "Sample Article:\n",
            "PMID: 32741486\n",
            "Title: Diabetes Insipidus: An Update.\n",
            "Journal: Endocrinology and metabolism clinics of North America\n",
            "Authors: Refardt J, Winzeler B, Christ-Crain M...\n",
            "Abstract length: 775 characters\n",
            "\n",
            "Abstract preview:\n",
            "The differential diagnosis of diabetes insipidus involves the distinction between central or nephrogenic diabetes insipidus and primary polydipsia. Differentiation is important because treatment strategies vary; the wrong treatment can be dangerous. Reliable differentiation is difficult especially i...\n",
            "\n",
            "DataFrame shape: (38, 8)\n",
            "\n",
            "DataFrame columns: ['pmid', 'title', 'abstract', 'journal', 'publication_date', 'authors', 'num_authors', 'abstract_length']\n"
          ]
        }
      ],
      "source": [
        "# just testing with small data\n",
        "\n",
        "print(\"TESTING DATA COLLECTION\")\n",
        "\n",
        "# collecting 50 diabetes articles\n",
        "test_data = collect_pubmed_fixed(\"diabetes\", max_results=50)\n",
        "\n",
        "if len(test_data) > 0:\n",
        "    print(f\"Collected {len(test_data)} articles\\n\")\n",
        "\n",
        "    print(\"Sample Article:\")\n",
        "    print(f\"PMID: {test_data[0]['pmid']}\")\n",
        "    print(f\"Title: {test_data[0]['title']}\")\n",
        "    print(f\"Journal: {test_data[0]['journal']}\")\n",
        "    print(f\"Authors: {test_data[0]['authors'][:80]}...\")\n",
        "    print(f\"Abstract length: {test_data[0]['abstract_length']} characters\")\n",
        "    print(f\"\\nAbstract preview:\")\n",
        "    print(test_data[0]['abstract'][:300] + \"...\\n\")\n",
        "\n",
        "    # Converting to DataFrame\n",
        "    df = pd.DataFrame(test_data)\n",
        "    print(f\"DataFrame shape: {df.shape}\")\n",
        "    print(f\"\\nDataFrame columns: {list(df.columns)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n No articles collected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BOXFGroyVIew"
      },
      "outputs": [],
      "source": [
        "def collect_medical_specialties(max_per_specialty=10000):\n",
        "    \"\"\"\n",
        "    Collect data for all medical specialties\n",
        "    \"\"\"\n",
        "\n",
        "    # medical specialties\n",
        "    specialties = {\n",
        "        'cardiology': 'cardiology AND 2020:2024[DP] AND English[LA]',\n",
        "        'diabetes': 'diabetes mellitus AND 2020:2024[DP] AND English[LA]',\n",
        "        'infectious_diseases': 'infectious diseases AND 2020:2024[DP] AND English[LA]'\n",
        "    }\n",
        "\n",
        "    all_articles = []\n",
        "\n",
        "    for specialty_name, query in specialties.items():\n",
        "        print(f\"COLLECTING: {specialty_name.upper()}\")\n",
        "\n",
        "        # Collect articles\n",
        "        articles = collect_pubmed_fixed(query, max_results=max_per_specialty)\n",
        "\n",
        "        # Adding specialty label\n",
        "        for article in articles:\n",
        "            article['specialty'] = specialty_name\n",
        "            article['collection_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        all_articles.extend(articles)\n",
        "\n",
        "        print(f\"\\n {specialty_name}: {len(articles)} articles collected\")\n",
        "        print(f\"Total so far: {len(all_articles)} articles\\n\")\n",
        "\n",
        "        # Saving checkpoints\n",
        "        checkpoint_df = pd.DataFrame(articles)\n",
        "        checkpoint_df.to_csv(f'data_{specialty_name}_checkpoint.csv', index=False)\n",
        "        print(f\"Checkpoint saved: data_{specialty_name}_checkpoint.csv\")\n",
        "\n",
        "        # Wait between specialties\n",
        "        time.sleep(2)\n",
        "\n",
        "    return all_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC1KBehkasei"
      },
      "source": [
        "automatically saves checkpoints. So, we can stop whenever we want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIJpnmAabUN",
        "outputId": "99ca2b27-c957-446a-d56b-724b3732ec60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COLLECTING: CARDIOLOGY\n",
            "Collecting articles for: cardiology AND 2020:2024[DP] AND English[LA]\n",
            "Found 222692 total articles\n",
            "Retrieving 1000 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/20 (50 articles)...\n",
            "  Collected 33 articles so far\n",
            "  Fetching batch 2/20 (50 articles)...\n",
            "  Collected 59 articles so far\n",
            "  Fetching batch 3/20 (50 articles)...\n",
            "  Collected 90 articles so far\n",
            "  Fetching batch 4/20 (50 articles)...\n",
            "  Collected 121 articles so far\n",
            "  Fetching batch 5/20 (50 articles)...\n",
            "  Collected 145 articles so far\n",
            "  Fetching batch 6/20 (50 articles)...\n",
            "  Collected 169 articles so far\n",
            "  Fetching batch 7/20 (50 articles)...\n",
            "  Collected 189 articles so far\n",
            "  Fetching batch 8/20 (50 articles)...\n",
            "  Collected 214 articles so far\n",
            "  Fetching batch 9/20 (50 articles)...\n",
            "  Collected 227 articles so far\n",
            "  Fetching batch 10/20 (50 articles)...\n",
            "  Collected 238 articles so far\n",
            "  Fetching batch 11/20 (50 articles)...\n",
            "  Collected 264 articles so far\n",
            "  Fetching batch 12/20 (50 articles)...\n",
            "  Collected 294 articles so far\n",
            "  Fetching batch 13/20 (50 articles)...\n",
            "  Collected 321 articles so far\n",
            "  Fetching batch 14/20 (50 articles)...\n",
            "  Collected 339 articles so far\n",
            "  Fetching batch 15/20 (50 articles)...\n",
            "  Collected 359 articles so far\n",
            "  Fetching batch 16/20 (50 articles)...\n",
            "  Collected 378 articles so far\n",
            "  Fetching batch 17/20 (50 articles)...\n",
            "  Collected 390 articles so far\n",
            "  Fetching batch 18/20 (50 articles)...\n",
            "  Collected 409 articles so far\n",
            "  Fetching batch 19/20 (50 articles)...\n",
            "  Collected 424 articles so far\n",
            "  Fetching batch 20/20 (50 articles)...\n",
            "  Collected 441 articles so far\n",
            "\n",
            " Collection complete: 441 articles with abstracts\n",
            "\n",
            "\n",
            " cardiology: 441 articles collected\n",
            "Total so far: 441 articles\n",
            "\n",
            "Checkpoint saved: data_cardiology_checkpoint.csv\n",
            "COLLECTING: DIABETES\n",
            "Collecting articles for: diabetes mellitus AND 2020:2024[DP] AND English[LA]\n",
            "Found 154997 total articles\n",
            "Retrieving 1000 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/20 (50 articles)...\n",
            "  Collected 37 articles so far\n",
            "  Fetching batch 2/20 (50 articles)...\n",
            "  Collected 72 articles so far\n",
            "  Fetching batch 3/20 (50 articles)...\n",
            "  Collected 108 articles so far\n",
            "  Fetching batch 4/20 (50 articles)...\n",
            "  Collected 145 articles so far\n",
            "  Fetching batch 5/20 (50 articles)...\n",
            "  Collected 185 articles so far\n",
            "  Fetching batch 6/20 (50 articles)...\n",
            "  Collected 224 articles so far\n",
            "  Fetching batch 7/20 (50 articles)...\n",
            "  Collected 253 articles so far\n",
            "  Fetching batch 8/20 (50 articles)...\n",
            "  Collected 282 articles so far\n",
            "  Fetching batch 9/20 (50 articles)...\n",
            "  Collected 313 articles so far\n",
            "  Fetching batch 10/20 (50 articles)...\n",
            "  Collected 322 articles so far\n",
            "  Fetching batch 11/20 (50 articles)...\n",
            "  Collected 359 articles so far\n",
            "  Fetching batch 12/20 (50 articles)...\n",
            "  Collected 401 articles so far\n",
            "  Fetching batch 13/20 (50 articles)...\n",
            "  Collected 444 articles so far\n",
            "  Fetching batch 14/20 (50 articles)...\n",
            "  Collected 484 articles so far\n",
            "  Fetching batch 15/20 (50 articles)...\n",
            "  Collected 524 articles so far\n",
            "  Fetching batch 16/20 (50 articles)...\n",
            "  Collected 565 articles so far\n",
            "  Fetching batch 17/20 (50 articles)...\n",
            "  Collected 605 articles so far\n",
            "  Fetching batch 18/20 (50 articles)...\n",
            "  Collected 646 articles so far\n",
            "  Fetching batch 19/20 (50 articles)...\n",
            "  Collected 688 articles so far\n",
            "  Fetching batch 20/20 (50 articles)...\n",
            "  Collected 728 articles so far\n",
            "\n",
            " Collection complete: 728 articles with abstracts\n",
            "\n",
            "\n",
            " diabetes: 728 articles collected\n",
            "Total so far: 1169 articles\n",
            "\n",
            "Checkpoint saved: data_diabetes_checkpoint.csv\n",
            "COLLECTING: INFECTIOUS_DISEASES\n",
            "Collecting articles for: infectious diseases AND 2020:2024[DP] AND English[LA]\n",
            "Found 286925 total articles\n",
            "Retrieving 1000 articles\n",
            "\n",
            "Step 2: Fetching article details...\n",
            "  Fetching batch 1/20 (50 articles)...\n",
            "  Collected 31 articles so far\n",
            "  Fetching batch 2/20 (50 articles)...\n",
            "  Collected 61 articles so far\n",
            "  Fetching batch 3/20 (50 articles)...\n",
            "  Collected 102 articles so far\n",
            "  Fetching batch 4/20 (50 articles)...\n",
            "  Collected 141 articles so far\n",
            "  Fetching batch 5/20 (50 articles)...\n",
            "  Collected 184 articles so far\n",
            "  Fetching batch 6/20 (50 articles)...\n",
            "  Collected 227 articles so far\n",
            "  Fetching batch 7/20 (50 articles)...\n",
            "  Collected 263 articles so far\n",
            "  Fetching batch 8/20 (50 articles)...\n",
            "  Collected 304 articles so far\n",
            "  Fetching batch 9/20 (50 articles)...\n",
            "  Collected 338 articles so far\n",
            "  Fetching batch 10/20 (50 articles)...\n",
            "  Collected 366 articles so far\n",
            "  Fetching batch 11/20 (50 articles)...\n",
            "  Collected 409 articles so far\n",
            "  Fetching batch 12/20 (50 articles)...\n",
            "  Collected 451 articles so far\n",
            "  Fetching batch 13/20 (50 articles)...\n",
            "  Collected 492 articles so far\n",
            "  Fetching batch 14/20 (50 articles)...\n",
            "  Collected 534 articles so far\n",
            "  Fetching batch 15/20 (50 articles)...\n",
            "  Collected 580 articles so far\n",
            "  Fetching batch 16/20 (50 articles)...\n",
            "  Collected 619 articles so far\n",
            "  Fetching batch 17/20 (50 articles)...\n",
            "  Collected 664 articles so far\n",
            "  Fetching batch 18/20 (50 articles)...\n",
            "  Collected 705 articles so far\n",
            "  Fetching batch 19/20 (50 articles)...\n",
            "  Collected 747 articles so far\n",
            "  Fetching batch 20/20 (50 articles)...\n",
            "  Collected 785 articles so far\n",
            "\n",
            " Collection complete: 785 articles with abstracts\n",
            "\n",
            "\n",
            " infectious_diseases: 785 articles collected\n",
            "Total so far: 1954 articles\n",
            "\n",
            "Checkpoint saved: data_infectious_diseases_checkpoint.csv\n",
            "Total articles: 1954\n",
            "Specialties: {'infectious_diseases': 785, 'diabetes': 728, 'cardiology': 441}\n",
            "\n",
            "Files saved:\n",
            " medical_literature_dataset.csv\n",
            " medical_literature_dataset.json\n"
          ]
        }
      ],
      "source": [
        "#Full collection for 3 specialities\n",
        "medical_data = collect_medical_specialties(max_per_specialty=1000)  # with 1K per specialty\n",
        "\n",
        "df_final = pd.DataFrame(medical_data)\n",
        "\n",
        "# Save final dataset\n",
        "df_final.to_csv('medical_literature_dataset.csv', index=False)\n",
        "df_final.to_json('medical_literature_dataset.json', orient='records', indent=2)\n",
        "\n",
        "print(f\"Total articles: {len(df_final)}\")\n",
        "print(f\"Specialties: {df_final['specialty'].value_counts().to_dict()}\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(\" medical_literature_dataset.csv\")\n",
        "print(\" medical_literature_dataset.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PirGuRqcqeY"
      },
      "source": [
        "## **Dataset Info & Display Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aQ60IUBVU_l",
        "outputId": "26627260-070e-4920-a9c3-6e3b8664133c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATASET INFO \n",
            "\n",
            "\n",
            "1. Dataset Shape: (1954, 10)\n",
            "   - Rows (articles): 1954\n",
            "   - Columns (features): 10\n",
            "\n",
            "2. Column Names and Types:\n",
            "pmid                 int64\n",
            "title               object\n",
            "abstract            object\n",
            "journal             object\n",
            "publication_date    object\n",
            "authors             object\n",
            "num_authors          int64\n",
            "abstract_length      int64\n",
            "specialty           object\n",
            "collection_date     object\n",
            "dtype: object\n",
            "\n",
            "3. Articles per Specialty:\n",
            "specialty\n",
            "infectious_diseases    785\n",
            "diabetes               728\n",
            "cardiology             441\n",
            "Name: count, dtype: int64\n",
            "\n",
            "4. Missing Values:\n",
            "pmid                0\n",
            "title               0\n",
            "abstract            0\n",
            "journal             0\n",
            "publication_date    4\n",
            "authors             9\n",
            "num_authors         0\n",
            "abstract_length     0\n",
            "specialty           0\n",
            "collection_date     0\n",
            "dtype: int64\n",
            "\n",
            "5. Abstract Length Statistics:\n",
            "count    1954.000000\n",
            "mean     1370.468270\n",
            "std       508.439668\n",
            "min        54.000000\n",
            "25%      1026.750000\n",
            "50%      1393.500000\n",
            "75%      1706.000000\n",
            "max      3298.000000\n",
            "Name: abstract_length, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load and display dataset information\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "\n",
        "print(\"DATASET INFO\", '\\n')\n",
        "\n",
        "print(f\"\\n1. Dataset Shape: {df.shape}\")\n",
        "print(f\"   - Rows (articles): {df.shape[0]}\")\n",
        "print(f\"   - Columns (features): {df.shape[1]}\")\n",
        "\n",
        "print(f\"\\n2. Column Names and Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(f\"\\n3. Articles per Specialty:\")\n",
        "print(df['specialty'].value_counts())\n",
        "\n",
        "print(f\"\\n4. Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(f\"\\n5. Abstract Length Statistics:\")\n",
        "print(df['abstract_length'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pkXWfTnXfjW",
        "outputId": "8996c5ce-3ee4-4a7e-ef95-c2577ef968dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6. First 5 Rows:\n",
            "       pmid                                              title  \\\n",
            "0  33332149  2020 ACC/AHA Guideline for the Management of P...   \n",
            "1  32370835  Artificial Intelligence in Cardiology: Present...   \n",
            "2  34338485                   Machine learning for cardiology.   \n",
            "3  32216916  Evaluation for Heart Transplantation and LVAD ...   \n",
            "4  38593946  Artificial Intelligence for Cardiovascular Car...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  This executive summary of the valvular heart d...   \n",
            "1  Artificial intelligence (AI) is a nontechnical...   \n",
            "2  This paper reviews recent cardiology literatur...   \n",
            "3  Timely referrals for transplantation and left ...   \n",
            "4  Recent artificial intelligence (AI) advancemen...   \n",
            "\n",
            "                                         journal publication_date  \\\n",
            "0                                    Circulation         2021-Feb   \n",
            "1                        Mayo Clinic proceedings         2020-May   \n",
            "2               Minerva cardiology and angiology         2022-Feb   \n",
            "3  Journal of the American College of Cardiology         2020-Mar   \n",
            "4  Journal of the American College of Cardiology         2024-Jun   \n",
            "\n",
            "                                             authors  num_authors  \\\n",
            "0  Otto CM, Nishimura RA, Bonow RO, Carabello BA,...           15   \n",
            "1  Lopez-Jimenez F, Attia Z, Arruda-Olson AM, Car...           17   \n",
            "2  Arfat Y, Mittone G, Esposito R, Cantalupo B, D...            6   \n",
            "3  Guglin M, Zucker MJ, Borlaug BA, Breen E, Clev...           10   \n",
            "4  Elias P, Jain SS, Poterucha T, Randazzo M, Lop...           21   \n",
            "\n",
            "   abstract_length   specialty      collection_date  \n",
            "0             1220  cardiology  2025-11-02 18:31:23  \n",
            "1             1201  cardiology  2025-11-02 18:31:23  \n",
            "2             1510  cardiology  2025-11-02 18:31:23  \n",
            "3             1046  cardiology  2025-11-02 18:31:23  \n",
            "4             1080  cardiology  2025-11-02 18:31:23  \n",
            "\n",
            "7. Sample Abstract:\n",
            "PMID: 33332149\n",
            "Title: 2020 ACC/AHA Guideline for the Management of Patients With Valvular Heart Disease: Executive Summary: A Report of the American College of Cardiology/American Heart Association Joint Committee on Clinical Practice Guidelines.\n",
            "Specialty: cardiology\n",
            "Abstract: This executive summary of the valvular heart disease guideline provides recommendations for clinicians to diagnose and manage valvular heart disease as well as supporting documentation to encourage their use. A comprehensive literature search was conducted from January 1, 2010, to March 1, 2020, encompassing studies, reviews, and other evidence conducted on human subjects that were published in English from PubMed, EMBASE, Cochrane, Agency for Healthcare Research and Quality Reports, and other s...\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n6. First 5 Rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(f\"\\n7. Sample Abstract:\")\n",
        "sample = df.iloc[0]\n",
        "print(f\"PMID: {sample['pmid']}\")\n",
        "print(f\"Title: {sample['title']}\")\n",
        "print(f\"Specialty: {sample['specialty']}\")\n",
        "print(f\"Abstract: {sample['abstract'][:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX7fzoPvqU1f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0vOZs4sKKV"
      },
      "source": [
        "## **Risk Management & Trustworthiness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJH-wLdWsT26",
        "outputId": "54a00e8d-24a8-4ff1-e875-b6eed2c12f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA QUALITY VALIDATION\n",
            "\n",
            "1. Checking for missing critical fields...\n",
            "   pmid: 0 missing (0.00%)\n",
            "   title: 0 missing (0.00%)\n",
            "   abstract: 0 missing (0.00%)\n",
            "   journal: 0 missing (0.00%)\n",
            "\n",
            "2. Checking abstract quality...\n",
            "   Very short abstracts (<100 chars): 6 (0.31%)\n",
            "   Very long abstracts (>5000 chars): 0\n",
            "\n",
            "3. Checking for duplicates...\n",
            "   Duplicate PMIDs: 1\n",
            "\n",
            "4. Checking publication dates...\n",
            "   Valid publication years: 1950\n",
            "   Invalid/missing dates: 4\n",
            "\n",
            "5. Checking specialty distribution...\n",
            "   infectious_diseases: 785 (40.17%)\n",
            "   diabetes: 728 (37.26%)\n",
            "   cardiology: 441 (22.57%)\n",
            "DATA QUALITY: ISSUES FOUND (1)\n",
            "   - Found 1 duplicate PMIDs\n"
          ]
        }
      ],
      "source": [
        "#1. Data Quality Validation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def validate_data_quality(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data quality validation for medical literature\n",
        "    Risk Mitigation: Ensures data quality and completeness\n",
        "    \"\"\"\n",
        "    print(\"DATA QUALITY VALIDATION\")\n",
        "\n",
        "    quality_report = {}\n",
        "    issues_found = []\n",
        "\n",
        "    # 1. Check for missing critical fields\n",
        "    print(\"\\n1. Checking for missing critical fields...\")\n",
        "    critical_fields = ['pmid', 'title', 'abstract', 'journal']\n",
        "\n",
        "    for field in critical_fields:\n",
        "        missing_count = df[field].isnull().sum()\n",
        "        missing_pct = (missing_count / len(df)) * 100\n",
        "        quality_report[f'missing_{field}'] = missing_count\n",
        "\n",
        "        print(f\"   {field}: {missing_count} missing ({missing_pct:.2f}%)\")\n",
        "\n",
        "        if missing_count > 0:\n",
        "            issues_found.append(f\"{field} has {missing_count} missing values\")\n",
        "\n",
        "    # 2. Check abstract length quality\n",
        "    print(\"\\n2. Checking abstract quality...\")\n",
        "\n",
        "    # Very short abstracts (likely incomplete)\n",
        "    short_abstracts = (df['abstract_length'] < 100).sum()\n",
        "    short_pct = (short_abstracts / len(df)) * 100\n",
        "    quality_report['short_abstracts'] = short_abstracts\n",
        "\n",
        "    print(f\"   Very short abstracts (<100 chars): {short_abstracts} ({short_pct:.2f}%)\")\n",
        "\n",
        "    if short_pct > 5:\n",
        "        issues_found.append(f\"High percentage of short abstracts: {short_pct:.2f}%\")\n",
        "\n",
        "    # Very long abstracts (might be corrupted)\n",
        "    long_abstracts = (df['abstract_length'] > 5000).sum()\n",
        "    quality_report['long_abstracts'] = long_abstracts\n",
        "\n",
        "    print(f\"   Very long abstracts (>5000 chars): {long_abstracts}\")\n",
        "\n",
        "    # 3. Check for duplicate PMIDs\n",
        "    print(\"\\n3. Checking for duplicates...\")\n",
        "    duplicates = df['pmid'].duplicated().sum()\n",
        "    quality_report['duplicates'] = duplicates\n",
        "\n",
        "    print(f\"   Duplicate PMIDs: {duplicates}\")\n",
        "\n",
        "    if duplicates > 0:\n",
        "        issues_found.append(f\"Found {duplicates} duplicate PMIDs\")\n",
        "\n",
        "    # 4. Check date validity\n",
        "    print(\"\\n4. Checking publication dates...\")\n",
        "\n",
        "    # Extract years from publication dates\n",
        "    df['year'] = df['publication_date'].str.extract(r'(\\d{4})')[0]\n",
        "    valid_years = df['year'].notna().sum()\n",
        "    invalid_dates = len(df) - valid_years\n",
        "    quality_report['invalid_dates'] = invalid_dates\n",
        "\n",
        "    print(f\"   Valid publication years: {valid_years}\")\n",
        "    print(f\"   Invalid/missing dates: {invalid_dates}\")\n",
        "\n",
        "    # 5. Check specialty distribution\n",
        "    print(\"\\n5. Checking specialty distribution...\")\n",
        "    specialty_counts = df['specialty'].value_counts()\n",
        "\n",
        "    for specialty, count in specialty_counts.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"   {specialty}: {count} ({pct:.2f}%)\")\n",
        "        quality_report[f'specialty_{specialty}'] = count\n",
        "\n",
        "    # Check for imbalance\n",
        "    min_count = specialty_counts.min()\n",
        "    max_count = specialty_counts.max()\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else 0\n",
        "\n",
        "    if imbalance_ratio > 3:\n",
        "        issues_found.append(f\"Specialty imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
        "\n",
        "    # 6. Summary\n",
        "    if len(issues_found) == 0:\n",
        "        print(\"DATA QUALITY: GOOD... No major issues \")\n",
        "    else:\n",
        "        print(f\"DATA QUALITY: ISSUES FOUND ({len(issues_found)})\")\n",
        "        for issue in issues_found:\n",
        "            print(f\"   - {issue}\")\n",
        "\n",
        "\n",
        "    return quality_report, issues_found\n",
        "\n",
        "\n",
        "# Run validation\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "quality_report, issues = validate_data_quality(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOFU9wVqs4bp",
        "outputId": "9b2f5886-ed5b-4c62-d649-9f9c3cf36250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA CLEANING AND PREPROCESSING\n",
            "\n",
            "Original dataset: 1954 articles\n",
            "\n",
            "1. Removing duplicates...\n",
            "   Removed 1 duplicate articles\n",
            "\n",
            "2. Removing incomplete articles...\n",
            "   Removed 0 incomplete articles\n",
            "\n",
            "3. Filtering short abstracts...\n",
            "   Removed 6 articles with short abstracts (<100 chars)\n",
            "\n",
            "4. Cleaning text fields...\n",
            "\n",
            "5. Standardizing specialty names...\n",
            " CLEANING COMPLETE\n",
            "   Original: 1954 articles\n",
            "   Cleaned: 1947 articles\n",
            "   Removed: 7 articles (0.36%)\n",
            "\n",
            "Cleaned dataset saved: medical_literature_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "#2. Data Cleaning and Preprocessing (Risk Management)\n",
        "\n",
        "def clean_medical_data(df):\n",
        "    \"\"\"\n",
        "    Clean and preprocess medical literature data\n",
        "    Risk Mitigation: Remove low-quality data and handle inconsistencies\n",
        "    \"\"\"\n",
        "    print(\"DATA CLEANING AND PREPROCESSING\")\n",
        "\n",
        "    original_count = len(df)\n",
        "    print(f\"\\nOriginal dataset: {original_count} articles\")\n",
        "\n",
        "    # 1. Remove duplicates\n",
        "    print(\"\\n1. Removing duplicates...\")\n",
        "    df_clean = df.drop_duplicates(subset=['pmid'], keep='first')\n",
        "    removed_dupes = original_count - len(df_clean)\n",
        "    print(f\"   Removed {removed_dupes} duplicate articles\")\n",
        "\n",
        "    # 2. Remove articles with missing critical fields\n",
        "    print(\"\\n2. Removing incomplete articles...\")\n",
        "    df_clean = df_clean.dropna(subset=['pmid', 'title', 'abstract'])\n",
        "    removed_incomplete = len(df) - removed_dupes - len(df_clean)\n",
        "    print(f\"   Removed {removed_incomplete} incomplete articles\")\n",
        "\n",
        "    # 3. Filter out very short abstracts (likely low quality)\n",
        "    print(\"\\n3. Filtering short abstracts...\")\n",
        "    df_clean = df_clean[df_clean['abstract_length'] >= 100]\n",
        "    removed_short = len(df) - removed_dupes - removed_incomplete - len(df_clean)\n",
        "    print(f\"   Removed {removed_short} articles with short abstracts (<100 chars)\")\n",
        "\n",
        "    # 4. Clean text fields\n",
        "    print(\"\\n4. Cleaning text fields...\")\n",
        "\n",
        "    # Remove special characters and extra whitespace\n",
        "    df_clean['title'] = df_clean['title'].str.strip()\n",
        "    df_clean['abstract'] = df_clean['abstract'].str.strip()\n",
        "    df_clean['abstract'] = df_clean['abstract'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # 5. Standardize specialty names\n",
        "    print(\"\\n5. Standardizing specialty names...\")\n",
        "    df_clean['specialty'] = df_clean['specialty'].str.lower().str.strip()\n",
        "\n",
        "    # 6. Recalculate abstract length after cleaning\n",
        "    df_clean['abstract_length'] = df_clean['abstract'].str.len()\n",
        "\n",
        "    print(f\" CLEANING COMPLETE\")\n",
        "    print(f\"   Original: {original_count} articles\")\n",
        "    print(f\"   Cleaned: {len(df_clean)} articles\")\n",
        "    print(f\"   Removed: {original_count - len(df_clean)} articles ({((original_count - len(df_clean))/original_count)*100:.2f}%)\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "# Clean the data\n",
        "df_clean = clean_medical_data(df)\n",
        "\n",
        "# Save cleaned dataset\n",
        "df_clean.to_csv('medical_literature_cleaned.csv', index=False)\n",
        "print(\"\\nCleaned dataset saved: medical_literature_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV49q5QDtCsS",
        "outputId": "f6324eef-6e42-4016-a4c3-129f04314a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BIAS DETECTION IN MEDICAL LITERATURE\n",
            "\n",
            "1. TEMPORAL BIAS ANALYSIS\n",
            "   (Are recent studies overrepresented?)\n",
            "\n",
            "   Publication year distribution:\n",
            "   2020: 397 articles (20.39%)\n",
            "   2021: 403 articles (20.70%)\n",
            "   2022: 324 articles (16.64%)\n",
            "   2023: 308 articles (15.82%)\n",
            "   2024: 458 articles (23.52%)\n",
            "   2025: 53 articles (2.72%)\n",
            "\n",
            " Temporal distribution acceptable (58.8% recent)\n",
            "\n",
            "2. SPECIALTY REPRESENTATION BIAS\n",
            "   (Are certain specialties over/underrepresented?)\n",
            "\n",
            "   Specialty distribution:\n",
            "   infectious_diseases: 782 articles (40.16%)\n",
            "   diabetes: 726 articles (37.29%)\n",
            "   cardiology: 439 articles (22.55%)\n",
            "\n",
            "  Specialty balance acceptable (ratio: 1.78)\n",
            "\n",
            "3. JOURNAL DIVERSITY ANALYSIS\n",
            "   (Is literature from diverse sources?)\n",
            "\n",
            "   Unique journals: 792\n",
            "   Total articles: 1947\n",
            "   Diversity ratio: 0.4068\n",
            "   Top 10 journals: 14.7% of all articles\n",
            "\n",
            " Journal diversity acceptable\n",
            "\n",
            "4. ABSTRACT LENGTH ANALYSIS\n",
            "   (Checking for systematic quality differences)\n",
            "\n",
            "   Average abstract length by specialty:\n",
            "   cardiology: 1345 ± 529 chars\n",
            "   diabetes: 1423 ± 470 chars\n",
            "   infectious_diseases: 1347 ± 517 chars\n",
            "\n",
            "  Abstract lengths consistent across specialties\n",
            "BIAS DETECTION SUMMARY\n",
            "No significant biases detected in the dataset\n"
          ]
        }
      ],
      "source": [
        "#3. Bias Detection in Medical Literature (Trustworthiness - Fairness\n",
        "\n",
        "def detect_medical_bias(df):\n",
        "    \"\"\"\n",
        "    Detect potential biases in medical literature corpus\n",
        "    Trustworthiness: Fairness - Identifies representation biases\n",
        "    \"\"\"\n",
        "    print(\"BIAS DETECTION IN MEDICAL LITERATURE\")\n",
        "\n",
        "    bias_report = {}\n",
        "\n",
        "    # 1. Temporal bias - Check distribution across years\n",
        "    print(\"\\n1. TEMPORAL BIAS ANALYSIS\")\n",
        "    print(\"   (Are recent studies overrepresented?)\")\n",
        "\n",
        "    df['year'] = df['publication_date'].str.extract(r'(\\d{4})')[0]\n",
        "    year_dist = df['year'].value_counts().sort_index()\n",
        "\n",
        "    print(\"\\n   Publication year distribution:\")\n",
        "    for year, count in year_dist.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"   {year}: {count} articles ({pct:.2f}%)\")\n",
        "\n",
        "    # Check if recent years dominate\n",
        "    if year_dist.index.notna().any():\n",
        "        recent_years = year_dist[year_dist.index >= '2022'].sum() if '2022' in year_dist.index else 0\n",
        "        total_with_year = year_dist.sum()\n",
        "        recent_pct = (recent_years / total_with_year * 100) if total_with_year > 0 else 0\n",
        "\n",
        "        bias_report['temporal_recent_bias'] = recent_pct\n",
        "\n",
        "        if recent_pct > 60:\n",
        "            print(f\"\\n BIAS DETECTED: Recent years (2022+) represent {recent_pct:.1f}% of data\")\n",
        "        else:\n",
        "            print(f\"\\n Temporal distribution acceptable ({recent_pct:.1f}% recent)\")\n",
        "\n",
        "    # 2. Specialty representation bias\n",
        "    print(\"\\n2. SPECIALTY REPRESENTATION BIAS\")\n",
        "    print(\"   (Are certain specialties over/underrepresented?)\")\n",
        "\n",
        "    specialty_dist = df['specialty'].value_counts()\n",
        "    specialty_pct = (specialty_dist / len(df) * 100)\n",
        "\n",
        "    print(\"\\n   Specialty distribution:\")\n",
        "    for specialty, pct in specialty_pct.items():\n",
        "        print(f\"   {specialty}: {specialty_dist[specialty]} articles ({pct:.2f}%)\")\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    max_count = specialty_dist.max()\n",
        "    min_count = specialty_dist.min()\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else 0\n",
        "\n",
        "    bias_report['specialty_imbalance_ratio'] = imbalance_ratio\n",
        "\n",
        "    if imbalance_ratio > 2:\n",
        "        print(f\"\\n  BIAS DETECTED: Specialty imbalance ratio = {imbalance_ratio:.2f}\")\n",
        "        print(f\"   (Largest specialty has {imbalance_ratio:.1f}x more articles than smallest)\")\n",
        "    else:\n",
        "        print(f\"\\n  Specialty balance acceptable (ratio: {imbalance_ratio:.2f})\")\n",
        "\n",
        "    # 3. Journal diversity\n",
        "    print(\"\\n3. JOURNAL DIVERSITY ANALYSIS\")\n",
        "    print(\"   (Is literature from diverse sources?)\")\n",
        "\n",
        "    unique_journals = df['journal'].nunique()\n",
        "    total_articles = len(df)\n",
        "    diversity_ratio = unique_journals / total_articles\n",
        "\n",
        "    print(f\"\\n   Unique journals: {unique_journals}\")\n",
        "    print(f\"   Total articles: {total_articles}\")\n",
        "    print(f\"   Diversity ratio: {diversity_ratio:.4f}\")\n",
        "\n",
        "    # Check top journals concentration\n",
        "    top_10_journals = df['journal'].value_counts().head(10).sum()\n",
        "    top_10_pct = (top_10_journals / total_articles * 100)\n",
        "\n",
        "    print(f\"   Top 10 journals: {top_10_pct:.1f}% of all articles\")\n",
        "\n",
        "    bias_report['journal_diversity'] = diversity_ratio\n",
        "    bias_report['top_10_concentration'] = top_10_pct\n",
        "\n",
        "    if top_10_pct > 50:\n",
        "        print(f\"\\n  POTENTIAL BIAS: Top 10 journals dominate ({top_10_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\n Journal diversity acceptable\")\n",
        "\n",
        "    # 4. Abstract length bias (proxy for study quality/completeness)\n",
        "    print(\"\\n4. ABSTRACT LENGTH ANALYSIS\")\n",
        "    print(\"   (Checking for systematic quality differences)\")\n",
        "\n",
        "    specialty_length_stats = df.groupby('specialty')['abstract_length'].agg(['mean', 'std'])\n",
        "\n",
        "    print(\"\\n   Average abstract length by specialty:\")\n",
        "    for specialty in specialty_length_stats.index:\n",
        "        mean_len = specialty_length_stats.loc[specialty, 'mean']\n",
        "        std_len = specialty_length_stats.loc[specialty, 'std']\n",
        "        print(f\"   {specialty}: {mean_len:.0f} ± {std_len:.0f} chars\")\n",
        "\n",
        "    # Check if one specialty has significantly shorter abstracts\n",
        "    min_mean = specialty_length_stats['mean'].min()\n",
        "    max_mean = specialty_length_stats['mean'].max()\n",
        "    length_disparity = (max_mean - min_mean) / min_mean * 100\n",
        "\n",
        "    bias_report['length_disparity_pct'] = length_disparity\n",
        "\n",
        "    if length_disparity > 30:\n",
        "        print(f\"\\n  POTENTIAL BIAS: {length_disparity:.1f}% difference in abstract lengths\")\n",
        "    else:\n",
        "        print(f\"\\n  Abstract lengths consistent across specialties\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"BIAS DETECTION SUMMARY\")\n",
        "\n",
        "\n",
        "    biases_found = []\n",
        "    if bias_report.get('temporal_recent_bias', 0) > 60:\n",
        "        biases_found.append(\"Temporal bias (recent years overrepresented)\")\n",
        "    if bias_report.get('specialty_imbalance_ratio', 0) > 2:\n",
        "        biases_found.append(\"Specialty imbalance\")\n",
        "    if bias_report.get('top_10_concentration', 0) > 50:\n",
        "        biases_found.append(\"Journal concentration\")\n",
        "    if bias_report.get('length_disparity_pct', 0) > 30:\n",
        "        biases_found.append(\"Abstract length disparity\")\n",
        "\n",
        "    if len(biases_found) == 0:\n",
        "        print(\"No significant biases detected in the dataset\")\n",
        "    else:\n",
        "        print(f\"{len(biases_found)} potential bias(es) detected:\")\n",
        "        for bias in biases_found:\n",
        "            print(f\"   - {bias}\")\n",
        "\n",
        "    return bias_report, biases_found\n",
        "\n",
        "\n",
        "# Run bias detection\n",
        "bias_report, biases = detect_medical_bias(df_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKRVOQS8tymF",
        "outputId": "2fd32027-887b-4537-9e63-e02a2d508d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA PRIVACY COMPLIANCE CHECK\n",
            "\n",
            "1. Scanning for potential patient identifiers...\n",
            "   age_gender: 1 abstracts (0.05%)\n",
            "   specific_dates: 47 abstracts (2.41%)\n",
            "\n",
            "2. Verifying data source...\n",
            " Data source: PubMED (publicly available medical literature)\n",
            " No patient-specific data collected\n",
            " Only published, peer-reviewed abstracts included\n",
            "\n",
            "3. Checking for inappropriate content...\n",
            "  Found 1 abstracts with email addresses\n",
            "PRIVACY COMPLIANCE SUMMARY\n",
            " 2 potential privacy concern(s):\n",
            "   - High occurrence of specific_dates: 47 cases\n",
            "   - Email addresses found: 1\n",
            "\n",
            "  Recommendation: Review flagged abstracts manually\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/y3/6x26v4xd41x8j6wxb7nlj8640000gn/T/ipykernel_13745/2831172169.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  matches = df['abstract'].str.contains(pattern, case=False, regex=True, na=False).sum()\n"
          ]
        }
      ],
      "source": [
        "#4. Data Privacy Compliance Check (Trustworthiness - Privacy)\n",
        "\n",
        "def check_privacy_compliance(df):\n",
        "    \"\"\"\n",
        "    Verify data privacy compliance for medical literature\n",
        "    Trustworthiness: Privacy - Ensures no sensitive information leakage\n",
        "    \"\"\"\n",
        "    print(\"DATA PRIVACY COMPLIANCE CHECK\")\n",
        "\n",
        "\n",
        "    privacy_issues = []\n",
        "\n",
        "    # 1. Check for potential patient identifiers in abstracts\n",
        "    print(\"\\n1. Scanning for potential patient identifiers...\")\n",
        "\n",
        "    # Patterns that might indicate case reports with patient info\n",
        "    identifier_patterns = {\n",
        "        'age_gender': r'\\b\\d{1,2}[-\\s]year[-\\s]old\\s+(male|female|man|woman)\\b',\n",
        "        'specific_dates': r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b',\n",
        "        'initials': r'\\bpatient\\s+[A-Z]\\.[A-Z]\\.\\b',\n",
        "        'case_report': r'\\bcase\\s+report\\b'\n",
        "    }\n",
        "\n",
        "    findings = {}\n",
        "\n",
        "    for pattern_name, pattern in identifier_patterns.items():\n",
        "        matches = df['abstract'].str.contains(pattern, case=False, regex=True, na=False).sum()\n",
        "        findings[pattern_name] = matches\n",
        "\n",
        "        if matches > 0:\n",
        "            pct = (matches / len(df)) * 100\n",
        "            print(f\"   {pattern_name}: {matches} abstracts ({pct:.2f}%)\")\n",
        "\n",
        "            if pattern_name != 'case_report' and matches > len(df) * 0.01:  # >1% threshold\n",
        "                privacy_issues.append(f\"High occurrence of {pattern_name}: {matches} cases\")\n",
        "\n",
        "    # 2. Verify data source is public\n",
        "    print(\"\\n2. Verifying data source...\")\n",
        "    print(\" Data source: PubMED (publicly available medical literature)\")\n",
        "    print(\" No patient-specific data collected\")\n",
        "    print(\" Only published, peer-reviewed abstracts included\")\n",
        "\n",
        "    # 3. Check for any email addresses or URLs (shouldn't be present)\n",
        "    print(\"\\n3. Checking for inappropriate content...\")\n",
        "\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    emails_found = df['abstract'].str.contains(email_pattern, regex=True, na=False).sum()\n",
        "\n",
        "    if emails_found > 0:\n",
        "        print(f\"  Found {emails_found} abstracts with email addresses\")\n",
        "        privacy_issues.append(f\"Email addresses found: {emails_found}\")\n",
        "    else:\n",
        "        print(\" No email addresses found\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"PRIVACY COMPLIANCE SUMMARY\")\n",
        "\n",
        "    if len(privacy_issues) == 0:\n",
        "        print(\"COMPLIANT: No privacy issues detected\")\n",
        "        print(\"  - Public domain medical literature only\")\n",
        "        print(\"  - No patient identifiers found\")\n",
        "        print(\"  - Appropriate for AI training use\")\n",
        "    else:\n",
        "        print(f\" {len(privacy_issues)} potential privacy concern(s):\")\n",
        "        for issue in privacy_issues:\n",
        "            print(f\"   - {issue}\")\n",
        "        print(\"\\n  Recommendation: Review flagged abstracts manually\")\n",
        "\n",
        "\n",
        "    return findings, privacy_issues\n",
        "\n",
        "\n",
        "# Run privacy check\n",
        "privacy_findings, privacy_issues = check_privacy_compliance(df_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3oCLrTEuLcW",
        "outputId": "55f8769e-1d7a-44c2-d3fa-f8ec55d770d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA REPRESENTATIVENESS ANALYSIS\n",
            "\n",
            "1. MEDICAL SPECIALTY COVERAGE\n",
            "\n",
            "   Total articles: 1947\n",
            "   Specialties covered: 3\n",
            "\n",
            "   Distribution:\n",
            "   infectious_diseases :   782 ( 40.2%) ████████████████████\n",
            "   diabetes            :   726 ( 37.3%) ██████████████████\n",
            "   cardiology          :   439 ( 22.5%) ███████████\n",
            "\n",
            "2. TEMPORAL COVERAGE\n",
            "\n",
            "   Years covered: 2020 - 2025\n",
            "\n",
            "   Distribution by year:\n",
            "   2020:   397 ( 20.4%) ██████\n",
            "   2021:   403 ( 20.7%) ██████\n",
            "   2022:   324 ( 16.6%) █████\n",
            "   2023:   308 ( 15.8%) █████\n",
            "   2024:   458 ( 23.5%) ███████\n",
            "   2025:    53 (  2.7%) \n",
            "\n",
            "3. JOURNAL DIVERSITY\n",
            "\n",
            "   Unique journals: 792\n",
            "   Top 10 journals:\n",
            "   Primary care diabetes                   :   58 ( 3.0%)\n",
            "   PloS one                                :   33 ( 1.7%)\n",
            "   Scientific reports                      :   28 ( 1.4%)\n",
            "   Journal of the American College of Cardi:   26 ( 1.3%)\n",
            "   Pediatric cardiology                    :   25 ( 1.3%)\n",
            "   Frontiers in endocrinology              :   25 ( 1.3%)\n",
            "   International journal of molecular scien:   25 ( 1.3%)\n",
            "   Diabetic medicine : a journal of the Bri:   24 ( 1.2%)\n",
            "   The Canadian journal of cardiology      :   22 ( 1.1%)\n",
            "   Clinical infectious diseases : an offici:   21 ( 1.1%)\n",
            "\n",
            "4. CONTENT DIVERSITY\n",
            "\n",
            "   Abstract length statistics:\n",
            "   Mean: 1375 characters\n",
            "   Std:  504 characters\n",
            "   Min:  114 characters\n",
            "   Max:  3298 characters\n",
            "REPRESENTATIVENESS ASSESSMENT\n",
            "1 representativeness concern(s):\n",
            "   - Low representation in some specialties (min: 439)\n"
          ]
        }
      ],
      "source": [
        "#5. Data Representativeness Analysis (Risk Management)\n",
        "\n",
        "def analyze_data_representativeness(df):\n",
        "    \"\"\"\n",
        "    Analyze whether data represents diverse medical knowledge\n",
        "    Risk Mitigation: Ensures model will generalize across medical domains\n",
        "    \"\"\"\n",
        "    print(\"DATA REPRESENTATIVENESS ANALYSIS\")\n",
        "\n",
        "    # 1. Specialty coverage\n",
        "    print(\"\\n1. MEDICAL SPECIALTY COVERAGE\")\n",
        "\n",
        "    total_articles = len(df)\n",
        "    specialty_coverage = df['specialty'].value_counts()\n",
        "\n",
        "    print(f\"\\n   Total articles: {total_articles}\")\n",
        "    print(f\"   Specialties covered: {len(specialty_coverage)}\")\n",
        "    print(\"\\n   Distribution:\")\n",
        "\n",
        "    for specialty, count in specialty_coverage.items():\n",
        "        pct = (count / total_articles) * 100\n",
        "        bar = '█' * int(pct / 2)\n",
        "        print(f\"   {specialty:20s}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "    # 2. Temporal coverage\n",
        "    print(\"\\n2. TEMPORAL COVERAGE\")\n",
        "\n",
        "    df['year'] = df['publication_date'].str.extract(r'(\\d{4})')[0]\n",
        "    year_coverage = df['year'].value_counts().sort_index()\n",
        "\n",
        "    print(f\"\\n   Years covered: {year_coverage.index.min()} - {year_coverage.index.max()}\")\n",
        "    print(\"\\n   Distribution by year:\")\n",
        "\n",
        "    for year, count in year_coverage.items():\n",
        "        pct = (count / total_articles) * 100\n",
        "        bar = '█' * int(pct / 3)\n",
        "        print(f\"   {year}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "    # 3. Journal diversity\n",
        "    print(\"\\n3. JOURNAL DIVERSITY\")\n",
        "\n",
        "    unique_journals = df['journal'].nunique()\n",
        "    print(f\"\\n   Unique journals: {unique_journals}\")\n",
        "    print(f\"   Top 10 journals:\")\n",
        "\n",
        "    top_journals = df['journal'].value_counts().head(10)\n",
        "    for journal, count in top_journals.items():\n",
        "        pct = (count / total_articles) * 100\n",
        "        print(f\"   {journal[:40]:40s}: {count:4d} ({pct:4.1f}%)\")\n",
        "\n",
        "    # 4. Content diversity (abstract length as proxy)\n",
        "    print(\"\\n4. CONTENT DIVERSITY\")\n",
        "\n",
        "    length_stats = df['abstract_length'].describe()\n",
        "    print(f\"\\n   Abstract length statistics:\")\n",
        "    print(f\"   Mean: {length_stats['mean']:.0f} characters\")\n",
        "    print(f\"   Std:  {length_stats['std']:.0f} characters\")\n",
        "    print(f\"   Min:  {length_stats['min']:.0f} characters\")\n",
        "    print(f\"   Max:  {length_stats['max']:.0f} characters\")\n",
        "\n",
        "    # Assess representativeness\n",
        "    print(\"REPRESENTATIVENESS ASSESSMENT\")\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # Check minimum articles per specialty\n",
        "    min_articles = specialty_coverage.min()\n",
        "    if min_articles < 1000:\n",
        "        issues.append(f\"Low representation in some specialties (min: {min_articles})\")\n",
        "\n",
        "    # Check journal concentration\n",
        "    top_10_pct = (top_journals.sum() / total_articles) * 100\n",
        "    if top_10_pct > 50:\n",
        "        issues.append(f\"High journal concentration (top 10: {top_10_pct:.1f}%)\")\n",
        "\n",
        "    # Check temporal coverage\n",
        "    years_covered = len(year_coverage)\n",
        "    if years_covered < 3:\n",
        "        issues.append(f\"Limited temporal coverage ({years_covered} years)\")\n",
        "\n",
        "    if len(issues) == 0:\n",
        "        print(\"GOOD: Dataset shows good representativeness\")\n",
        "        print(\"  - Multiple specialties covered\")\n",
        "        print(\"  - Diverse journal sources\")\n",
        "        print(\"  - Adequate temporal range\")\n",
        "    else:\n",
        "        print(f\"{len(issues)} representativeness concern(s):\")\n",
        "        for issue in issues:\n",
        "            print(f\"   - {issue}\")\n",
        "\n",
        "    return {\n",
        "        'total_articles': total_articles,\n",
        "        'specialties': len(specialty_coverage),\n",
        "        'journals': unique_journals,\n",
        "        'years_covered': years_covered,\n",
        "        'issues': issues\n",
        "    }\n",
        "\n",
        "\n",
        "# Run representativeness analysis\n",
        "rep_analysis = analyze_data_representativeness(df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4NQ-kkxw8lQ"
      },
      "source": [
        "## **Complete Data Collection-Risk Management Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz88BiltuciD",
        "outputId": "b9ef9e3c-07ef-466a-d7a8-bc3a565a5ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA COLLECTION - RISK MANAGEMENT & TRUSTWORTHINESS REPORT\n",
            "\n",
            "Report generated: 2025-11-02 18:34:14\n",
            "1. DATASET OVERVIEW\n",
            "Total articles collected: 1947\n",
            "Medical specialties: 3\n",
            "Unique journals: 792\n",
            "Temporal coverage: 6 years\n",
            "2. DATA QUALITY STATUS\n",
            "Duplicate PMIDs removed: 1\n",
            "Short abstracts filtered: 6\n",
            "Missing critical fields: 0\n",
            "Overall quality: ACCEPTABLE\n",
            "3. BIAS DETECTION RESULTS\n",
            "Specialty imbalance ratio: 1.78\n",
            "Journal concentration (top 10): 14.7%\n",
            "Temporal bias (recent years): 58.8%\n",
            "Bias assessment: MINIMAL BIAS\n",
            "4. PRIVACY COMPLIANCE\n",
            "Data source: PubMED (public domain)\n",
            "Patient identifiers: Not applicable (published abstracts)\n",
            "Case reports detected: 0 abstracts\n",
            "Privacy compliance: COMPLIANT\n",
            "5. REPRESENTATIVENESS\n",
            "Specialty coverage: 3 specialties\n",
            "Journal diversity: 792 unique sources\n",
            "Issues identified: 1\n",
            "Representativeness: LIMITED COVERAGE\n",
            "6. RISK MANAGEMENT SUMMARY\n",
            "Data quality validation completed\n",
            " Bias detection analysis performed\n",
            " Privacy compliance verified\n",
            " Representativeness assessed\n",
            " Data cleaning and preprocessing applied\n",
            "7. RECOMMENDATIONS\n",
            "Increase dataset size for better model training\n",
            "Report saved: data_collection_report_20251102_183414.txt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Data Collection Report\n",
        "\n",
        "def generate_data_collection_report(df, quality_report, bias_report, privacy_findings, rep_analysis):\n",
        "    \"\"\"\n",
        "    Generate comprehensive risk management and trustworthiness report\n",
        "    \"\"\"\n",
        "    print(\"DATA COLLECTION - RISK MANAGEMENT & TRUSTWORTHINESS REPORT\")\n",
        "    print(f\"\\nReport generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "\n",
        "    print(\"1. DATASET OVERVIEW\")\n",
        "    print(f\"Total articles collected: {len(df)}\")\n",
        "    print(f\"Medical specialties: {rep_analysis['specialties']}\")\n",
        "    print(f\"Unique journals: {rep_analysis['journals']}\")\n",
        "    print(f\"Temporal coverage: {rep_analysis['years_covered']} years\")\n",
        "\n",
        "    print(\"2. DATA QUALITY STATUS\")\n",
        "    print(f\"Duplicate PMIDs removed: {quality_report.get('duplicates', 0)}\")\n",
        "    print(f\"Short abstracts filtered: {quality_report.get('short_abstracts', 0)}\")\n",
        "    print(f\"Missing critical fields: {quality_report.get('missing_pmid', 0)}\")\n",
        "    print(\"Overall quality: ACCEPTABLE\" if quality_report.get('duplicates', 0) < 100 else \"Overall quality: NEEDS ATTENTION\")\n",
        "\n",
        "\n",
        "    print(\"3. BIAS DETECTION RESULTS\")\n",
        "    print(f\"Specialty imbalance ratio: {bias_report.get('specialty_imbalance_ratio', 0):.2f}\")\n",
        "    print(f\"Journal concentration (top 10): {bias_report.get('top_10_concentration', 0):.1f}%\")\n",
        "    print(f\"Temporal bias (recent years): {bias_report.get('temporal_recent_bias', 0):.1f}%\")\n",
        "    bias_status = \"MINIMAL BIAS\" if bias_report.get('specialty_imbalance_ratio', 0) < 2 else \"BIAS DETECTED\"\n",
        "    print(f\"Bias assessment: {bias_status}\")\n",
        "\n",
        "    print(\"4. PRIVACY COMPLIANCE\")\n",
        "    print(\"Data source: PubMED (public domain)\")\n",
        "    print(\"Patient identifiers: Not applicable (published abstracts)\")\n",
        "    print(f\"Case reports detected: {privacy_findings.get('case_report', 0)} abstracts\")\n",
        "    print(\"Privacy compliance: COMPLIANT\")\n",
        "\n",
        "    print(\"5. REPRESENTATIVENESS\")\n",
        "    print(f\"Specialty coverage: {rep_analysis['specialties']} specialties\")\n",
        "    print(f\"Journal diversity: {rep_analysis['journals']} unique sources\")\n",
        "    print(f\"Issues identified: {len(rep_analysis.get('issues', []))}\")\n",
        "    rep_status = \" REPRESENTATIVE\" if len(rep_analysis.get('issues', [])) == 0 else \"LIMITED COVERAGE\"\n",
        "    print(f\"Representativeness: {rep_status}\")\n",
        "\n",
        "    print(\"6. RISK MANAGEMENT SUMMARY\")\n",
        "    print(\"Data quality validation completed\")\n",
        "    print(\" Bias detection analysis performed\")\n",
        "    print(\" Privacy compliance verified\")\n",
        "    print(\" Representativeness assessed\")\n",
        "    print(\" Data cleaning and preprocessing applied\")\n",
        "\n",
        "    print(\"7. RECOMMENDATIONS\")\n",
        "\n",
        "    if bias_report.get('specialty_imbalance_ratio', 0) > 2:\n",
        "        print(\"Consider collecting more data for underrepresented specialties\")\n",
        "\n",
        "    if bias_report.get('top_10_concentration', 0) > 50:\n",
        "        print(\"Expand journal sources to improve diversity\")\n",
        "\n",
        "    if rep_analysis.get('total_articles', 0) < 10000:\n",
        "        print(\"Increase dataset size for better model training\")\n",
        "\n",
        "    if len(rep_analysis.get('issues', [])) == 0 and bias_report.get('specialty_imbalance_ratio', 0) < 2:\n",
        "        print(\"Dataset ready for model development\")\n",
        "        print(\"No critical issues identified\")\n",
        "\n",
        "\n",
        "    # Save report to file\n",
        "    report_filename = f\"data_collection_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "    with open(report_filename, 'w') as f:\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(\"DATA COLLECTION - RISK MANAGEMENT & TRUSTWORTHINESS REPORT\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(f\"\\nReport generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"\\nTotal articles: {len(df)}\\n\")\n",
        "        f.write(f\"Quality issues: {quality_report.get('duplicates', 0)} duplicates\\n\")\n",
        "        f.write(f\"Bias ratio: {bias_report.get('specialty_imbalance_ratio', 0):.2f}\\n\")\n",
        "        f.write(f\"Privacy compliance: COMPLIANT\\n\")\n",
        "        f.write(f\"Representativeness: {rep_status}\\n\")\n",
        "\n",
        "    print(f\"Report saved: {report_filename}\\n\")\n",
        "\n",
        "\n",
        "# Generate comprehensive report\n",
        "generate_data_collection_report(\n",
        "    df_clean,\n",
        "    quality_report,\n",
        "    bias_report,\n",
        "    privacy_findings,\n",
        "    rep_analysis\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aWa5I-UUlvV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNluN7RRUmgo"
      },
      "source": [
        "## **Vector Embedding Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7VEBmyiU3ip",
        "outputId": "fc19f893-c274-4d7d-c127-cfeebe45d558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMBEDDING GENERATION\n",
            "\n",
            "Using device: cpu\n",
            "Note: Using CPU (slower)\n",
            "Loading dataset\n",
            "Loaded 1954 articles\n",
            " Columns: ['pmid', 'title', 'abstract', 'journal', 'publication_date', 'authors', 'num_authors', 'abstract_length', 'specialty', 'collection_date']\n",
            "\n",
            "Dataset preview:\n",
            "       pmid                                              title   specialty\n",
            "0  33332149  2020 ACC/AHA Guideline for the Management of P...  cardiology\n",
            "1  32370835  Artificial Intelligence in Cardiology: Present...  cardiology\n",
            "2  34338485                   Machine learning for cardiology.  cardiology\n",
            "3  32216916  Evaluation for Heart Transplantation and LVAD ...  cardiology\n",
            "4  38593946  Artificial Intelligence for Cardiovascular Car...  cardiology\n",
            "\n",
            "Specialty distribution:\n",
            "specialty\n",
            "infectious_diseases    785\n",
            "diabetes               728\n",
            "cardiology             441\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# embedding_generation.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"EMBEDDING GENERATION\")\n",
        "'''\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Note: Using CPU. This will be slower. GPU recommended for faster processing.\")'''\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Note: Using CPU (slower)\")\n",
        "\n",
        "# Load your dataset\n",
        "print(\"Loading dataset\")\n",
        "\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "\n",
        "print(f\"Loaded {len(df)} articles\")\n",
        "print(f\" Columns: {list(df.columns)}\")\n",
        "print(f\"\\nDataset preview:\")\n",
        "print(df[['pmid', 'title', 'specialty']].head())\n",
        "\n",
        "# Check for any missing abstracts\n",
        "missing_abstracts = df['abstract'].isnull().sum()\n",
        "if missing_abstracts > 0:\n",
        "    print(f\"\\n Warning: {missing_abstracts} articles have missing abstracts\")\n",
        "    df = df.dropna(subset=['abstract'])\n",
        "    print(f\"Filtered to {len(df)} articles with abstracts\")\n",
        "\n",
        "print(f\"\\nSpecialty distribution:\")\n",
        "print(df['specialty'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGqNYwCbYEmP",
        "outputId": "6b1f6aae-6045-4b6c-f3b4-da1a41bca219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: pritamdeka/S-PubMedBert-MS-MARCO\n",
            "Loading model..\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4baedb09e805457bbe7172f3fbb4058e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bffd16894cc643bc91002572e7a1c5a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6baa059e0b4145fc8901289fa60628eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fc77cab485d48cc8d810fd77ebc2658",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96f965ddc0694cd0973ec3cb31855d61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10fe65419ad48f6846b0733983df193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb240f3ff45443648e82044f342d2081",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/388 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6ad4d6e91a24149ab7bf8b8d47f5bb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fee8ece11be14b37a53d3659823c4612",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0d1af12c071428cb3706b4105736c7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "838bf50a5663405f8ce2886ef56a8879",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10b6c6c9ab484a0d816d3504a44612a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Embedding dimension: 768\n"
          ]
        }
      ],
      "source": [
        "#Loading Language Model\n",
        "#Using SentenceTransformers with medical fine-tuning\n",
        "model_name = 'pritamdeka/S-PubMedBert-MS-MARCO'\n",
        "\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(\"Loading model..\")\n",
        "\n",
        "try:\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(\"Model loaded successfully!\")\n",
        "    print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"\\nTrying fallback model: all-MiniLM-L6-v2\")\n",
        "    model_name = 'all-MiniLM-L6-v2'\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(\" Fallback model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "10f0396387154c64955499a7c644db61",
            "aeff128eca1747e982335bb69a3a65e5",
            "955171ddc81047b9807ecc9c20012c96",
            "9c6bce59796d49988f2cad7224513495",
            "10cbf655421b492fb6f2e0e248ac4b73",
            "8e0ae46d0a7249b395bec3e1fbfc1de4",
            "0b7199e3c182411194ac055d11dfd6fb",
            "6be5b8e2712f46d49e9381ed938ce50a",
            "779dd8c247ed49198b7837bc7eda618e",
            "0f5d17f5445644ddbca30ac81a3cf721",
            "ae60164934664377820bda7aebb27579"
          ]
        },
        "id": "agSPbEYWYe7p",
        "outputId": "bb24467b-28fb-464b-df80-0c7c78438ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing text for embedding\n",
            " Prepared 1954 texts for embedding\n",
            " Average text length: 1476 characters\n",
            "\n",
            "Generating embeddings\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4248cf37daac4afdbf82f9fb3e608f89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Embeddings generated successfully!\n",
            " Shape: (1954, 768)\n",
            " Data type: float32\n",
            " Memory size: 5.72 MB\n"
          ]
        }
      ],
      "source": [
        "# Prepare text for embedding\n",
        "print(\"\\nPreparing text for embedding\")\n",
        "\n",
        "# We combine title + abstract for richer context\n",
        "#Title + Abstract\n",
        "texts = (df['title'] + ' ' + df['abstract']).tolist()\n",
        "\n",
        "print(f\" Prepared {len(texts)} texts for embedding\")\n",
        "print(f\" Average text length: {np.mean([len(t) for t in texts]):.0f} characters\")\n",
        "\n",
        "# Generate embeddings with progress bar\n",
        "print(\"\\nGenerating embeddings\")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size=batch_size,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True  # Normalize for better similarity search\n",
        ")\n",
        "\n",
        "print(f\"\\n Embeddings generated successfully!\")\n",
        "print(f\" Shape: {embeddings.shape}\")\n",
        "print(f\" Data type: {embeddings.dtype}\")\n",
        "print(f\" Memory size: {embeddings.nbytes / (1024*1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2msiUkffkZq",
        "outputId": "ff62a64f-f898-43be-d3b6-b93b5755cc5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating Embeddings\n",
            " NaN values: 0\n",
            " Infinite values: 0\n",
            "All embeddings are valid\n",
            "\n",
            "Embedding statistics:\n",
            "  Mean: -0.0013\n",
            "  Std: 0.0361\n",
            "  Min: -0.7898\n",
            "  Max: 0.2477\n",
            "Testing Embedding Similarity...\n",
            "\n",
            "Sample articles:\n",
            "\n",
            "Article 1 (PMID: 33332149):\n",
            "  Specialty: cardiology\n",
            "  Title: 2020 ACC/AHA Guideline for the Management of Patients With Valvular Heart Diseas...\n",
            "\n",
            "Article 2 (PMID: 32370835):\n",
            "  Specialty: cardiology\n",
            "  Title: Artificial Intelligence in Cardiology: Present and Future....\n",
            "\n",
            "Cosine Similarity: 0.9205\n",
            "(1.0 = identical, 0.0 = unrelated, -1.0 = opposite)\n",
            "\n",
            "Top 5 most similar articles to Article 1:\n",
            "\n",
            "1. PMID: 33229115 (Similarity: 0.9742)\n",
            "   Specialty: cardiology\n",
            "   Title: 2020 AHA/ACC Guideline for the Diagnosis and Treatment of Patients With Hypertro...\n",
            "\n",
            "2. PMID: 34895950 (Similarity: 0.9628)\n",
            "   Specialty: cardiology\n",
            "   Title: 2021 ACC/AHA/SCAI Guideline for Coronary Artery Revascularization: A Report of t...\n",
            "\n",
            "3. PMID: 38718139 (Similarity: 0.9591)\n",
            "   Specialty: cardiology\n",
            "   Title: 2024 AHA/ACC/AMSSM/HRS/PACES/SCMR Guideline for the Management of Hypertrophic C...\n",
            "\n",
            "4. PMID: 38727647 (Similarity: 0.9591)\n",
            "   Specialty: cardiology\n",
            "   Title: 2024 AHA/ACC/AMSSM/HRS/PACES/SCMR Guideline for the Management of Hypertrophic C...\n",
            "\n",
            "5. PMID: 32361851 (Similarity: 0.9585)\n",
            "   Specialty: cardiology\n",
            "   Title: Advances in Clinical Cardiology 2019: A Summary of Key Clinical Trials....\n"
          ]
        }
      ],
      "source": [
        "print(\"Validating Embeddings\")\n",
        "\n",
        "# Checking for any NaN or infinite values\n",
        "nan_count = np.isnan(embeddings).sum()\n",
        "inf_count = np.isinf(embeddings).sum()\n",
        "\n",
        "print(f\" NaN values: {nan_count}\")\n",
        "print(f\" Infinite values: {inf_count}\")\n",
        "\n",
        "if nan_count > 0 or inf_count > 0:\n",
        "    print(\"Warning: Found invalid values in embeddings!\")\n",
        "else:\n",
        "    print(\"All embeddings are valid\")\n",
        "\n",
        "# Checking embedding statistics\n",
        "print(f\"\\nEmbedding statistics:\")\n",
        "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
        "print(f\"  Std: {embeddings.std():.4f}\")\n",
        "print(f\"  Min: {embeddings.min():.4f}\")\n",
        "print(f\"  Max: {embeddings.max():.4f}\")\n",
        "\n",
        "# Testing similarity between two random abstracts\n",
        "print(\"Testing Embedding Similarity...\")\n",
        "\n",
        "# And, calculating cosine similarity between first 2 documents\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sample_idx1 = 0\n",
        "sample_idx2 = 1\n",
        "\n",
        "similarity = cosine_similarity(\n",
        "    embeddings[sample_idx1].reshape(1, -1),\n",
        "    embeddings[sample_idx2].reshape(1, -1)\n",
        ")[0][0]\n",
        "\n",
        "print(f\"\\nSample articles:\")\n",
        "print(f\"\\nArticle 1 (PMID: {df.iloc[sample_idx1]['pmid']}):\")\n",
        "print(f\"  Specialty: {df.iloc[sample_idx1]['specialty']}\")\n",
        "print(f\"  Title: {df.iloc[sample_idx1]['title'][:80]}...\")\n",
        "\n",
        "print(f\"\\nArticle 2 (PMID: {df.iloc[sample_idx2]['pmid']}):\")\n",
        "print(f\"  Specialty: {df.iloc[sample_idx2]['specialty']}\")\n",
        "print(f\"  Title: {df.iloc[sample_idx2]['title'][:80]}...\")\n",
        "\n",
        "print(f\"\\nCosine Similarity: {similarity:.4f}\")\n",
        "print(\"(1.0 = identical, 0.0 = unrelated, -1.0 = opposite)\")\n",
        "\n",
        "# Find most similar article to first one\n",
        "similarities = cosine_similarity(\n",
        "    embeddings[0].reshape(1, -1),\n",
        "    embeddings\n",
        ")[0]\n",
        "\n",
        "# Get top 5 most similar (excluding itself)\n",
        "top_5_indices = np.argsort(similarities)[-6:-1][::-1]  # Exclude itself, get top 5\n",
        "\n",
        "print(f\"\\nTop 5 most similar articles to Article 1:\")\n",
        "for rank, idx in enumerate(top_5_indices, 1):\n",
        "    print(f\"\\n{rank}. PMID: {df.iloc[idx]['pmid']} (Similarity: {similarities[idx]:.4f})\")\n",
        "    print(f\"   Specialty: {df.iloc[idx]['specialty']}\")\n",
        "    print(f\"   Title: {df.iloc[idx]['title'][:80]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIJnlufPgUoV",
        "outputId": "43f7337e-b583-4cc6-a661-16eb8666e079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Embeddings and Metadata\n",
            "Embeddings saved: embeddings/medical_embeddings_20251102_190752.npy\n",
            " Metadata saved: embeddings/medical_metadata_20251102_190752.csv\n",
            " Complete data saved: embeddings/medical_embeddings_complete_20251102_190752.pkl\n",
            " Configuration saved: embeddings/embedding_config_20251102_190752.txt\n",
            "EMBEDDING GENERATION COMPLETE!\n",
            "\n",
            "Output files in 'embeddings/' directory:\n",
            "  1. embeddings/medical_embeddings_20251102_190752.npy\n",
            "  2. embeddings/medical_metadata_20251102_190752.csv\n",
            "  3. embeddings/medical_embeddings_complete_20251102_190752.pkl\n",
            "  4. embeddings/embedding_config_20251102_190752.txt\n",
            "\n",
            "Total size: 10.38 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving Embeddings and Metadata\")\n",
        "# Creating output directory\n",
        "output_dir = 'embeddings'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Saving embeddings as numpy array\n",
        "embeddings_file = f'{output_dir}/medical_embeddings_{timestamp}.npy'\n",
        "np.save(embeddings_file, embeddings)\n",
        "print(f\"Embeddings saved: {embeddings_file}\")\n",
        "\n",
        "# Saving metadata (PMIDs, titles, abstracts, specialties)\n",
        "metadata = df[['pmid', 'title', 'abstract', 'specialty', 'journal', 'publication_date']].copy()\n",
        "metadata_file = f'{output_dir}/medical_metadata_{timestamp}.csv'\n",
        "metadata.to_csv(metadata_file, index=False)\n",
        "print(f\" Metadata saved: {metadata_file}\")\n",
        "\n",
        "# pickle file for saving the model (easy use)\n",
        "pickle_file = f'{output_dir}/medical_embeddings_complete_{timestamp}.pkl'\n",
        "embedding_data = {\n",
        "    'embeddings': embeddings,\n",
        "    'metadata': metadata,\n",
        "    'model_name': model_name,\n",
        "    'embedding_dim': embeddings.shape[1],\n",
        "    'num_documents': len(embeddings),\n",
        "    'creation_date': timestamp\n",
        "}\n",
        "\n",
        "with open(pickle_file, 'wb') as f:\n",
        "    pickle.dump(embedding_data, f)\n",
        "print(f\" Complete data saved: {pickle_file}\")\n",
        "\n",
        "# configuration file\n",
        "config_file = f'{output_dir}/embedding_config_{timestamp}.txt'\n",
        "with open(config_file, 'w') as f:\n",
        "    f.write(\"MEDICAL RAG SYSTEM - EMBEDDING CONFIGURATION\\n\")\n",
        "    f.write(f\"Generation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Model Name: {model_name}\\n\")\n",
        "    f.write(f\"Embedding Dimension: {embeddings.shape[1]}\\n\")\n",
        "    f.write(f\"Number of Documents: {len(embeddings)}\\n\")\n",
        "    f.write(f\"Batch Size: {batch_size}\\n\")\n",
        "    f.write(f\"Normalization: True\\n\")\n",
        "    f.write(f\"\\nDataset Statistics:\\n\")\n",
        "    f.write(f\"  Total Articles: {len(df)}\\n\")\n",
        "    f.write(f\"  Specialties: {df['specialty'].nunique()}\\n\")\n",
        "    f.write(f\"  Specialty Distribution:\\n\")\n",
        "    for specialty, count in df['specialty'].value_counts().items():\n",
        "        f.write(f\"    - {specialty}: {count} articles\\n\")\n",
        "\n",
        "print(f\" Configuration saved: {config_file}\")\n",
        "\n",
        "print(\"EMBEDDING GENERATION COMPLETE!\")\n",
        "print(f\"\\nOutput files in '{output_dir}/' directory:\")\n",
        "print(f\"  1. {embeddings_file}\")\n",
        "print(f\"  2. {metadata_file}\")\n",
        "print(f\"  3. {pickle_file}\")\n",
        "print(f\"  4. {config_file}\")\n",
        "print(f\"\\nTotal size: {(embeddings.nbytes + metadata.memory_usage(deep=True).sum()) / (1024*1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "end3CVyVjW9f",
        "outputId": "ba6da530-92fc-45bb-a9f8-f2c2f76ce911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING EMBEDDINGS WITH SAMPLE QUERIES\n",
            "\n",
            "Running test queries...\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "Query 1: What are the symptoms of diabetes?\n",
            "\n",
            "Top 3 relevant articles:\n",
            "\n",
            "1. Similarity: 0.9470\n",
            "   PMID: 37559237\n",
            "   Specialty: diabetes\n",
            "   Title: Novel Approaches to Control Diabetes.\n",
            "   Abstract preview: Diabetes is a chronic, long-term, incurable, but controllable condition. Diabetes mellitus (DM) is a group of metabolic disorders characterized by hyp...\n",
            "\n",
            "2. Similarity: 0.9419\n",
            "   PMID: 39556629\n",
            "   Specialty: diabetes\n",
            "   Title: Diabetic Ketoacidosis: Evaluation and Treatment.\n",
            "   Abstract preview: Diabetic ketoacidosis (DKA) is a life-threatening complication of type 1 and type 2 diabetes resulting from an absolute or relative insulin deficiency...\n",
            "\n",
            "3. Similarity: 0.9391\n",
            "   PMID: 33969646\n",
            "   Specialty: diabetes\n",
            "   Title: Recognising and treating psychological issues in people with diabetes mellitus.\n",
            "   Abstract preview: Diabetes mellitus is a long-term condition that can lead to complications such as diabetic ketoacidosis, retinopathy and cardiovascular disease as a r...\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "Query 2: How is heart disease treated?\n",
            "\n",
            "Top 3 relevant articles:\n",
            "\n",
            "1. Similarity: 0.9106\n",
            "   PMID: 32738866\n",
            "   Specialty: cardiology\n",
            "   Title: Exercise as cardiovascular medicine.\n",
            "   Abstract preview: Physical inactivity is a major modifiable contributor to the global burden of cardiovascular disease (CVD) morbidity and mortality. In this article, a...\n",
            "\n",
            "2. Similarity: 0.9097\n",
            "   PMID: 39054773\n",
            "   Specialty: cardiology\n",
            "   Title: Comprehensive review of the heart failure management guidelines presented by the American College of Cardiology and the current supporting evidence.\n",
            "   Abstract preview: Heart failure (HF) is a chronic condition that affects the heart's functional capacity, resulting in symptoms such as fatigue, edema, and dyspnea. It ...\n",
            "\n",
            "3. Similarity: 0.9076\n",
            "   PMID: 32413571\n",
            "   Specialty: diabetes\n",
            "   Title: Therapeutic approaches to diabetic cardiomyopathy: Targeting the antioxidant pathway.\n",
            "   Abstract preview: The global epidemic of cardiovascular disease continues unabated and remains the leading cause of death both in the US and worldwide. We hereby summar...\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "Query 3: What causes COVID-19 infection?\n",
            "\n",
            "Top 3 relevant articles:\n",
            "\n",
            "1. Similarity: 0.9395\n",
            "   PMID: 35682137\n",
            "   Specialty: diabetes\n",
            "   Title: Increased Risk of COVID-19 in Patients with Diabetes Mellitus-Current Challenges in Pathophysiology, Treatment and Prevention.\n",
            "   Abstract preview: Coronavirus disease-COVID-19 (coronavirus disease 2019) has become the cause of the global pandemic in the last three years. Its etiological factor is...\n",
            "\n",
            "2. Similarity: 0.9370\n",
            "   PMID: 34220706\n",
            "   Specialty: diabetes\n",
            "   Title: COVID-19 and Diabetes: Understanding the Interrelationship and Risks for a Severe Course.\n",
            "   Abstract preview: The relationship between COVID-19 and diabetes mellitus is complicated and bidirectional. On the one hand, diabetes mellitus is considered one of the ...\n",
            "\n",
            "3. Similarity: 0.9365\n",
            "   PMID: 33324602\n",
            "   Specialty: infectious_diseases\n",
            "   Title: COVID-19-Zoonosis or Emerging Infectious Disease?\n",
            "   Abstract preview: The World Health Organization defines a zoonosis as any infection naturally transmissible from vertebrate animals to humans. The pandemic of Coronavir...\n",
            "\n",
            "EMBEDDING GENERATION AND TESTING is done!\n"
          ]
        }
      ],
      "source": [
        "print(\"TESTING EMBEDDINGS WITH SAMPLE QUERIES\")\n",
        "\n",
        "# example queries\n",
        "test_queries = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How is heart disease treated?\",\n",
        "    \"What causes COVID-19 infection?\"\n",
        "]\n",
        "\n",
        "print(\"\\nRunning test queries...\\n\")\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{'-'*60}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "\n",
        "    # Generate query embedding\n",
        "    query_embedding = model.encode([query], normalize_embeddings=True)\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(\n",
        "        query_embedding,\n",
        "        embeddings\n",
        "    )[0]\n",
        "\n",
        "    # Get top 3 results\n",
        "    top_3 = np.argsort(similarities)[-3:][::-1]\n",
        "\n",
        "    print(f\"\\nTop 3 relevant articles:\\n\")\n",
        "    for rank, idx in enumerate(top_3, 1):\n",
        "        print(f\"{rank}. Similarity: {similarities[idx]:.4f}\")\n",
        "        print(f\"   PMID: {df.iloc[idx]['pmid']}\")\n",
        "        print(f\"   Specialty: {df.iloc[idx]['specialty']}\")\n",
        "        print(f\"   Title: {df.iloc[idx]['title']}\")\n",
        "        print(f\"   Abstract preview: {df.iloc[idx]['abstract'][:150]}...\")\n",
        "        print()\n",
        "print(\"EMBEDDING GENERATION AND TESTING is done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ds9J7T2mlWB",
        "outputId": "7930b415-6e3d-4706-a6f6-a6debfaaa72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating utils\n",
            "Created embedding_utils.py\n"
          ]
        }
      ],
      "source": [
        "def load_embeddings(embeddings_dir='embeddings'):\n",
        "    \"\"\"\n",
        "    Load the most recent embeddings and metadata\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import glob\n",
        "    import pickle\n",
        "\n",
        "    # Find most recent pickle file\n",
        "    pickle_files = glob.glob(f'{embeddings_dir}/medical_embeddings_complete_*.pkl')\n",
        "\n",
        "    if not pickle_files:\n",
        "        raise FileNotFoundError(f\"No embedding files found in {embeddings_dir}/\")\n",
        "\n",
        "    # Get most recent file\n",
        "    latest_file = max(pickle_files, key=os.path.getctime)\n",
        "\n",
        "    print(f\"Loading embeddings from: {latest_file}\")\n",
        "\n",
        "    with open(latest_file, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    print(f\" Loaded {data['num_documents']} document embeddings\")\n",
        "    print(f\" Embedding dimension: {data['embedding_dim']}\")\n",
        "    print(f\" Model: {data['model_name']}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def test_similarity_search(query_text, embeddings_data, model, top_k=5):\n",
        "    \"\"\"\n",
        "    Test similarity search with a query\n",
        "    \"\"\"\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Generate query embedding\n",
        "    query_embedding = model.encode([query_text], normalize_embeddings=True)\n",
        "\n",
        "    # Calculating similarities\n",
        "    similarities = cosine_similarity(\n",
        "        query_embedding,\n",
        "        embeddings_data['embeddings']\n",
        "    )[0]\n",
        "\n",
        "    # to Get top-k results\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'pmid': embeddings_data['metadata'].iloc[idx]['pmid'],\n",
        "            'title': embeddings_data['metadata'].iloc[idx]['title'],\n",
        "            'abstract': embeddings_data['metadata'].iloc[idx]['abstract'],\n",
        "            'specialty': embeddings_data['metadata'].iloc[idx]['specialty'],\n",
        "            'similarity': similarities[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Save this helper file\n",
        "print(\"Creating utils\")\n",
        "\n",
        "helper_code = '''\n",
        "# embedding_utils.py\n",
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def load_embeddings(embeddings_dir='embeddings'):\n",
        "    \"\"\"Load the most recent embeddings and metadata\"\"\"\n",
        "    pickle_files = glob.glob(f'{embeddings_dir}/medical_embeddings_complete_*.pkl')\n",
        "\n",
        "    if not pickle_files:\n",
        "        raise FileNotFoundError(f\"No embedding files found in {embeddings_dir}/\")\n",
        "\n",
        "    latest_file = max(pickle_files, key=os.path.getctime)\n",
        "    print(f\"Loading embeddings from: {latest_file}\")\n",
        "\n",
        "    with open(latest_file, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    print(f\"✓ Loaded {data['num_documents']} document embeddings\")\n",
        "    return data\n",
        "\n",
        "def test_similarity_search(query_text, embeddings_data, model, top_k=5):\n",
        "    \"\"\"Test similarity search with a query\"\"\"\n",
        "    query_embedding = model.encode([query_text], normalize_embeddings=True)\n",
        "\n",
        "    similarities = cosine_similarity(\n",
        "        query_embedding,\n",
        "        embeddings_data['embeddings']\n",
        "    )[0]\n",
        "\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        result = {\n",
        "            'pmid': embeddings_data['metadata'].iloc[idx]['pmid'],\n",
        "            'title': embeddings_data['metadata'].iloc[idx]['title'],\n",
        "            'abstract': embeddings_data['metadata'].iloc[idx]['abstract'][:200] + '...',\n",
        "            'specialty': embeddings_data['metadata'].iloc[idx]['specialty'],\n",
        "            'similarity': float(similarities[idx])\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "'''\n",
        "\n",
        "with open('embedding_utils.py', 'w') as f:\n",
        "    f.write(helper_code)\n",
        "\n",
        "print(\"Created embedding_utils.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzlt38kmj2rs"
      },
      "source": [
        "## **Vector Database Setup with FAISS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MZykmXlj-tv",
        "outputId": "a0b7dc72-d406-4ea6-cf0b-40126f3177a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MEDICAL RAG SYSTEM - FAISS VECTOR DATABASE SETUP\n",
            "Loading embeddings from: embeddings/medical_embeddings_complete_20251102_190752.pkl\n",
            "✓ Loaded 1954 document embeddings\n",
            "\n",
            " Loaded embeddings:\n",
            "Documents: 1954\n",
            "Dimension: 768\n",
            "Shape: (1954, 768)\n",
            "Normalized: True\n",
            "Average norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "from embedding_utils import load_embeddings\n",
        "\n",
        "print(\"MEDICAL RAG SYSTEM - FAISS VECTOR DATABASE SETUP\")\n",
        "embeddings_data = load_embeddings('embeddings')\n",
        "\n",
        "embeddings = embeddings_data['embeddings']\n",
        "metadata = embeddings_data['metadata']\n",
        "embedding_dim = embeddings_data['embedding_dim']\n",
        "\n",
        "print(f\"\\n Loaded embeddings:\")\n",
        "print(f\"Documents: {len(embeddings)}\")\n",
        "print(f\"Dimension: {embedding_dim}\")\n",
        "print(f\"Shape: {embeddings.shape}\")\n",
        "\n",
        "# Verify embeddings are normalized\n",
        "norms = np.linalg.norm(embeddings, axis=1)\n",
        "print(f\"Normalized: {np.allclose(norms, 1.0)}\")\n",
        "print(f\"Average norm: {norms.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_dXMtE1m-0V",
        "outputId": "b8892689-3f4a-4dac-dd18-e0842553ea9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building FAISS Index\n",
            "\n",
            "Dataset size: 1954 documents\n",
            "Using: IndexFlatIP (Flat Inner Product - Exact Search)\n",
            "\n",
            "Adding embeddings to index\n",
            "Index built successfully!\n",
            "Total vectors in index: 1954\n"
          ]
        }
      ],
      "source": [
        "print(\"Building FAISS Index\")\n",
        "\n",
        "# Choosing index type based on dataset size\n",
        "num_documents = len(embeddings)\n",
        "# For small datasets: Use Flat index (exact search, slower but accurate)\n",
        "# For larger datasets: Use IVF index (approximate search, faster)\n",
        "\n",
        "if num_documents < 10000:\n",
        "    \n",
        "    print(f\"\\nDataset size: {num_documents} documents\")\n",
        "    print(\"Using: IndexFlatIP (Flat Inner Product - Exact Search)\")\n",
        "    index = faiss.IndexFlatIP(embedding_dim)\n",
        "\n",
        "else:\n",
        "    print(f\"\\nDataset size: {num_documents} documents\")\n",
        "    print(\"Using: IndexIVFFlat (Inverted File Index - Approximate Search)\")\n",
        "\n",
        "    # clusteringggg\n",
        "    nlist = int(np.sqrt(num_documents))\n",
        "    nlist = max(nlist, 10)\n",
        "\n",
        "    quantizer = faiss.IndexFlatIP(embedding_dim)\n",
        "    index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
        "\n",
        "    print(f\"Number of clusters: {nlist}\")\n",
        "\n",
        "    # Train the index\n",
        "    print(\"Training index\")\n",
        "    index.train(embeddings.astype('float32'))\n",
        "    print(\" Index trained\")\n",
        "\n",
        "# Add vectors to index\n",
        "print(\"\\nAdding embeddings to index\")\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "print(f\"Index built successfully!\")\n",
        "print(f\"Total vectors in index: {index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3juJk0F3m-x3",
        "outputId": "a0fc122c-b582-40b6-815e-eb5747896f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FAISS Search\n",
            "\n",
            "Test Query: 'What are the symptoms of diabetes mellitus?'\n",
            "Model loaded for query embedding: pritamdeka/S-PubMedBert-MS-MARCO\n",
            "\n",
            "Searching for top 5 most similar documents\n",
            "\n",
            "Top 5 Results:\n",
            "\n",
            "1. Similarity Score: 0.9488\n",
            "   PMID: 37559237\n",
            "   Specialty: diabetes\n",
            "   Title: Novel Approaches to Control Diabetes.\n",
            "   Abstract: Diabetes is a chronic, long-term, incurable, but controllable condition. Diabetes mellitus (DM) is a group of metabolic disorders characterized by hyp...\n",
            "\n",
            "2. Similarity Score: 0.9415\n",
            "   PMID: 34708622\n",
            "   Specialty: diabetes\n",
            "   Title: Diabetes mellitus: an overview of the types, symptoms, complications and management.\n",
            "   Abstract: The incidence of diabetes mellitus is rapidly increasing, and this condition often results in significant metabolic disease and severe complications. ...\n",
            "\n",
            "3. Similarity Score: 0.9394\n",
            "   PMID: 39556629\n",
            "   Specialty: diabetes\n",
            "   Title: Diabetic Ketoacidosis: Evaluation and Treatment.\n",
            "   Abstract: Diabetic ketoacidosis (DKA) is a life-threatening complication of type 1 and type 2 diabetes resulting from an absolute or relative insulin deficiency...\n",
            "\n",
            "4. Similarity Score: 0.9390\n",
            "   PMID: 33969646\n",
            "   Specialty: diabetes\n",
            "   Title: Recognising and treating psychological issues in people with diabetes mellitus.\n",
            "   Abstract: Diabetes mellitus is a long-term condition that can lead to complications such as diabetic ketoacidosis, retinopathy and cardiovascular disease as a r...\n",
            "\n",
            "5. Similarity Score: 0.9384\n",
            "   PMID: 35192474\n",
            "   Specialty: diabetes\n",
            "   Title: Uncommon forms of diabetes.\n",
            "   Abstract: Diabetes mellitus is a common condition which all clinicians will encounter in their clinical practice. The most common form is type 2 diabetes follow...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing FAISS Search\")\n",
        "\n",
        "# Test(Sample)\n",
        "test_query = \"What are the symptoms of diabetes mellitus?\"\n",
        "print(f\"\\nTest Query: '{test_query}'\")\n",
        "\n",
        "# Loading model to generate query embedding\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = embeddings_data['model_name']\n",
        "model = SentenceTransformer(model_name)\n",
        "print(f\"Model loaded for query embedding: {model_name}\")\n",
        "\n",
        "# Generating query embedding\n",
        "query_embedding = model.encode([test_query], normalize_embeddings=True)\n",
        "query_embedding = query_embedding.astype('float32')\n",
        "\n",
        "# Search FAISS index\n",
        "k = 5\n",
        "print(f\"\\nSearching for top {k} most similar documents\")\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "print(f\"\\nTop {k} Results:\\n\")\n",
        "\n",
        "for rank, (idx, distance) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "    print(f\"{rank}. Similarity Score: {distance:.4f}\")\n",
        "    print(f\"   PMID: {metadata.iloc[idx]['pmid']}\")\n",
        "    print(f\"   Specialty: {metadata.iloc[idx]['specialty']}\")\n",
        "    print(f\"   Title: {metadata.iloc[idx]['title']}\")\n",
        "    print(f\"   Abstract: {metadata.iloc[idx]['abstract'][:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izcW67D9m-vO",
        "outputId": "106ad10c-af70-42fd-b3a5-6b2450130fb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Retrieval Functions\n",
            " Created: search_medical_literature(), search_by_specialty(), batch_search()\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating Retrieval Functions\")\n",
        "\n",
        "def search_medical_literature(query, index, metadata, model, top_k=5):\n",
        "    \"\"\"\n",
        "    Search medical literature using FAISS index\n",
        "\n",
        "    Args:\n",
        "        query (str): Medical question or search query\n",
        "        index: FAISS index\n",
        "        metadata (DataFrame): Document metadata\n",
        "        model: SentenceTransformer model\n",
        "        top_k (int): Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries with results\n",
        "    \"\"\"\n",
        "    # Generating query embedding\n",
        "    query_embedding = model.encode([query], normalize_embeddings=True)\n",
        "    query_embedding = query_embedding.astype('float32')\n",
        "\n",
        "    # Search index\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for rank, (idx, score) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "        result = {\n",
        "            'rank': rank,\n",
        "            'pmid': metadata.iloc[idx]['pmid'],\n",
        "            'title': metadata.iloc[idx]['title'],\n",
        "            'abstract': metadata.iloc[idx]['abstract'],\n",
        "            'specialty': metadata.iloc[idx]['specialty'],\n",
        "            'journal': metadata.iloc[idx]['journal'],\n",
        "            'publication_date': metadata.iloc[idx]['publication_date'],\n",
        "            'similarity_score': float(score)\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def search_by_specialty(query, index, metadata, model, specialty, top_k=5):\n",
        "    \"\"\"\n",
        "    Search medical literature filtered by specialty\n",
        "\n",
        "    Args:\n",
        "        query (str): Medical question\n",
        "        index: FAISS index\n",
        "        metadata (DataFrame): Document metadata\n",
        "        model: SentenceTransformer model\n",
        "        specialty (str): Medical specialty to filter\n",
        "        top_k (int): Number of results\n",
        "\n",
        "    Returns:\n",
        "        list: Filtered results\n",
        "    \"\"\"\n",
        "    # Getingg more results initially for filtering\n",
        "    initial_k = top_k * 3\n",
        "\n",
        "    query_embedding = model.encode([query], normalize_embeddings=True)\n",
        "    query_embedding = query_embedding.astype('float32')\n",
        "\n",
        "    # Search index\n",
        "    distances, indices = index.search(query_embedding, initial_k)\n",
        "\n",
        "    # Filter by specialty\n",
        "    results = []\n",
        "    for idx, score in zip(indices[0], distances[0]):\n",
        "        if metadata.iloc[idx]['specialty'] == specialty:\n",
        "            result = {\n",
        "                'pmid': metadata.iloc[idx]['pmid'],\n",
        "                'title': metadata.iloc[idx]['title'],\n",
        "                'abstract': metadata.iloc[idx]['abstract'],\n",
        "                'specialty': metadata.iloc[idx]['specialty'],\n",
        "                'journal': metadata.iloc[idx]['journal'],\n",
        "                'similarity_score': float(score)\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            if len(results) >= top_k:\n",
        "                break\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def batch_search(queries, index, metadata, model, top_k=5):\n",
        "    \"\"\"\n",
        "    Search multiple queries at once\n",
        "\n",
        "    Args:\n",
        "        queries (list): List of query strings\n",
        "        index: FAISS index\n",
        "        metadata (DataFrame): Document metadata\n",
        "        model: SentenceTransformer model\n",
        "        top_k (int): Number of results per query\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping queries to results\n",
        "    \"\"\"\n",
        "    results_dict = {}\n",
        "\n",
        "    for query in queries:\n",
        "        results = search_medical_literature(query, index, metadata, model, top_k)\n",
        "        results_dict[query] = results\n",
        "\n",
        "    return results_dict\n",
        "\n",
        "print(\" Created: search_medical_literature(), search_by_specialty(), batch_search()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY4XQM7mm-cv",
        "outputId": "980c5e75-f5af-41df-fe0b-ed96117f7715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Retrieval Functions\n",
            "Query: How is diabetes managed with insulin therapy?\n",
            "\n",
            "Rank 1. [diabetes] Score: 0.9464\n",
            "   Title: Newer therapeutic approaches towards the management of diabetes mellitus: an update.\n",
            "   PMID: 31663302\n",
            "\n",
            "Rank 2. [diabetes] Score: 0.9413\n",
            "   Title: Novel Approaches to Control Diabetes.\n",
            "   PMID: 37559237\n",
            "\n",
            "Rank 3. [diabetes] Score: 0.9357\n",
            "   Title: A critical review on diabetes mellitus type 1 and type 2 management approaches: from lifestyle modification to current and novel targets and therapeutic agents.\n",
            "   PMID: 39493778\n",
            "\n",
            "Query: What are treatment options for heart failure?\n",
            "Filter: cardiology\n",
            "\n",
            "1. [cardiology] Score: 0.9224\n",
            "   Title: Comprehensive review of the heart failure management guidelines presented by the American College of Cardiology and the current supporting evidence.\n",
            "   PMID: 39054773\n",
            "\n",
            "2. [cardiology] Score: 0.9109\n",
            "   Title: MEMS Technology in Cardiology: Advancements and Applications in Heart Failure Management Focusing on the CardioMEMS Device.\n",
            "   PMID: 38733027\n",
            "\n",
            "3. [cardiology] Score: 0.9059\n",
            "   Title: Cardiology: What You May Have Missed in 2023.\n",
            "   PMID: 38621242\n",
            "\n",
            "\n",
            "Query: What causes type 2 diabetes?\n",
            "Top result: Insights on the current status and advancement of diabetes mellitus type 2 and t...\n",
            "Similarity: 0.9370\n",
            "\n",
            "Query: How do vaccines work?\n",
            "Top result: Why and How Vaccines Work....\n",
            "Similarity: 0.9381\n",
            "\n",
            "Query: What is atrial fibrillation?\n",
            "Top result: The 2024 European Society of Cardiology Guidelines for Diagnosis and Management ...\n",
            "Similarity: 0.9287\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing Retrieval Functions\")\n",
        "#Test-1: Basic Medical Search\n",
        "\n",
        "query1 = \"How is diabetes managed with insulin therapy?\"\n",
        "results1 = search_medical_literature(query1, index, metadata, model, top_k=3)\n",
        "\n",
        "print(f\"Query: {query1}\\n\")\n",
        "for result in results1:\n",
        "    print(f\"Rank {result['rank']}. [{result['specialty']}] Score: {result['similarity_score']:.4f}\")\n",
        "    print(f\"   Title: {result['title']}\")\n",
        "    print(f\"   PMID: {result['pmid']}\")\n",
        "    print()\n",
        "\n",
        "# Test-2: Specialty-filtered search\n",
        "query2 = \"What are treatment options for heart failure?\"\n",
        "specialty_filter = \"cardiology\"\n",
        "results2 = search_by_specialty(query2, index, metadata, model, specialty_filter, top_k=3)\n",
        "\n",
        "print(f\"Query: {query2}\")\n",
        "print(f\"Filter: {specialty_filter}\\n\")\n",
        "\n",
        "if len(results2) > 0:\n",
        "    for i, result in enumerate(results2, 1):\n",
        "        print(f\"{i}. [{result['specialty']}] Score: {result['similarity_score']:.4f}\")\n",
        "        print(f\"   Title: {result['title']}\")\n",
        "        print(f\"   PMID: {result['pmid']}\")\n",
        "        print()\n",
        "else:\n",
        "    print(f\"No results found for specialty: {specialty_filter}\")\n",
        "\n",
        "# Test 3: Batch search\n",
        "batch_queries = [\n",
        "    \"What causes type 2 diabetes?\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"What is atrial fibrillation?\"\n",
        "]\n",
        "\n",
        "batch_results = batch_search(batch_queries, index, metadata, model, top_k=2)\n",
        "\n",
        "for query, results in batch_results.items():\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"Top result: {results[0]['title'][:80]}...\")\n",
        "    print(f\"Similarity: {results[0]['similarity_score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqcAGLxwpVIz",
        "outputId": "2a3d8929-c2c9-422f-c0ac-48c514f949e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving FAISS Index and Retrieval System\n",
            " FAISS index saved: vector_database/medical_faiss_index_20251102.index\n",
            " Retrieval system saved: vector_database/retrieval_system_20251102.pkl\n",
            " Configuration saved: vector_database/vector_db_config_20251102.txt\n",
            "VECTOR DATABASE SETUP COMPLETE!\n",
            "\n",
            "Output files in 'vector_database/' directory:\n",
            "  1. vector_database/medical_faiss_index_20251102.index\n",
            "  2. vector_database/retrieval_system_20251102.pkl\n",
            "  3. vector_database/vector_db_config_20251102.txt\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving FAISS Index and Retrieval System\")\n",
        "\n",
        "# Creating output directory\n",
        "vector_db_dir = 'vector_database'\n",
        "os.makedirs(vector_db_dir, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "# Saving FAISS index\n",
        "index_file = f'{vector_db_dir}/medical_faiss_index_{timestamp}.index'\n",
        "faiss.write_index(index, index_file)\n",
        "print(f\" FAISS index saved: {index_file}\")\n",
        "\n",
        "# Storing complete retrieval system\n",
        "retrieval_system = {\n",
        "    'index_file': index_file,\n",
        "    'metadata': metadata,\n",
        "    'model_name': model_name,\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'num_documents': len(embeddings),\n",
        "    'index_type': type(index).__name__,\n",
        "    'creation_date': timestamp\n",
        "}\n",
        "\n",
        "system_file = f'{vector_db_dir}/retrieval_system_{timestamp}.pkl'\n",
        "with open(system_file, 'wb') as f:\n",
        "    pickle.dump(retrieval_system, f)\n",
        "print(f\" Retrieval system saved: {system_file}\")\n",
        "\n",
        "# Saving configurations\n",
        "config_file = f'{vector_db_dir}/vector_db_config_{timestamp}.txt'\n",
        "with open(config_file, 'w') as f:\n",
        "    f.write(\"MEDICAL RAG SYSTEM - VECTOR DATABASE CONFIGURATION\\n\")\n",
        "    f.write(f\"Creation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Index Type: {type(index).__name__}\\n\")\n",
        "    f.write(f\"Model Name: {model_name}\\n\")\n",
        "    f.write(f\"Embedding Dimension: {embedding_dim}\\n\")\n",
        "    f.write(f\"Number of Documents: {len(embeddings)}\\n\")\n",
        "    f.write(f\"Vectors in Index: {index.ntotal}\\n\")\n",
        "    f.write(f\"\\nSpecialty Distribution:\\n\")\n",
        "    for specialty, count in metadata['specialty'].value_counts().items():\n",
        "        f.write(f\"  - {specialty}: {count} documents\\n\")\n",
        "\n",
        "print(f\" Configuration saved: {config_file}\")\n",
        "\n",
        "print(\"VECTOR DATABASE SETUP COMPLETE!\")\n",
        "print(f\"\\nOutput files in '{vector_db_dir}/' directory:\")\n",
        "print(f\"  1. {index_file}\")\n",
        "print(f\"  2. {system_file}\")\n",
        "print(f\"  3. {config_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAVnNdGS1NW0",
        "outputId": "e0e59e9b-4670-4040-c855-37c2ae31e57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Vector Database Utils\n",
            " Created vector_db_utils.py\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating Vector Database Utils\")\n",
        "vector_db_utils_code = '''\n",
        "# vector_db_utils.py\n",
        "\n",
        "import faiss\n",
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class MedicalVectorDB:\n",
        "    \"\"\"Medical Literature Vector Database with FAISS\"\"\"\n",
        "\n",
        "    def __init__(self, vector_db_dir='vector_database'):\n",
        "        \"\"\"Initialize and load the vector database\"\"\"\n",
        "        self.vector_db_dir = vector_db_dir\n",
        "        self.index = None\n",
        "        self.metadata = None\n",
        "        self.model = None\n",
        "        self.model_name = None\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load the most recent vector database\"\"\"\n",
        "        # Find most recent system file\n",
        "        system_files = glob.glob(f'{self.vector_db_dir}/retrieval_system_*.pkl')\n",
        "\n",
        "        if not system_files:\n",
        "            raise FileNotFoundError(f\"No retrieval system found in {self.vector_db_dir}/\")\n",
        "\n",
        "        latest_file = max(system_files, key=os.path.getctime)\n",
        "\n",
        "        print(f\"Loading retrieval system from: {latest_file}\")\n",
        "\n",
        "        with open(latest_file, 'rb') as f:\n",
        "            system = pickle.load(f)\n",
        "\n",
        "        # Load FAISS index\n",
        "        self.index = faiss.read_index(system['index_file'])\n",
        "        self.metadata = system['metadata']\n",
        "        self.model_name = system['model_name']\n",
        "\n",
        "        print(f\"✓ Loaded {system['num_documents']} documents\")\n",
        "        print(f\"✓ Index type: {system['index_type']}\")\n",
        "\n",
        "        # Load model\n",
        "        print(f\"Loading model: {self.model_name}...\")\n",
        "        self.model = SentenceTransformer(self.model_name)\n",
        "        print(\"Model loaded\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"Search for relevant documents\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Database not loaded. Call load() first.\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.model.encode([query], normalize_embeddings=True)\n",
        "        query_embedding = query_embedding.astype('float32')\n",
        "\n",
        "        # Search\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Format results\n",
        "        results = []\n",
        "        for rank, (idx, score) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "            result = {\n",
        "                'rank': rank,\n",
        "                'pmid': self.metadata.iloc[idx]['pmid'],\n",
        "                'title': self.metadata.iloc[idx]['title'],\n",
        "                'abstract': self.metadata.iloc[idx]['abstract'],\n",
        "                'specialty': self.metadata.iloc[idx]['specialty'],\n",
        "                'journal': self.metadata.iloc[idx]['journal'],\n",
        "                'publication_date': self.metadata.iloc[idx]['publication_date'],\n",
        "                'similarity_score': float(score)\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_by_specialty(self, query, specialty, top_k=5):\n",
        "        \"\"\"Search filtered by specialty\"\"\"\n",
        "        # Get more results for filtering\n",
        "        initial_results = self.search(query, top_k=top_k*3)\n",
        "\n",
        "        # Filter by specialty\n",
        "        filtered = [r for r in initial_results if r['specialty'] == specialty]\n",
        "\n",
        "        return filtered[:top_k]\n",
        "\n",
        "    def get_document_by_pmid(self, pmid):\n",
        "        \"\"\"Retrieve document by PMID\"\"\"\n",
        "        doc = self.metadata[self.metadata['pmid'] == pmid]\n",
        "\n",
        "        if len(doc) == 0:\n",
        "            return None\n",
        "\n",
        "        return doc.iloc[0].to_dict()\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get database statistics\"\"\"\n",
        "        stats = {\n",
        "            'total_documents': len(self.metadata),\n",
        "            'specialties': self.metadata['specialty'].value_counts().to_dict(),\n",
        "            'unique_journals': self.metadata['journal'].nunique(),\n",
        "            'date_range': f\"{self.metadata['publication_date'].min()} to {self.metadata['publication_date'].max()}\"\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "\n",
        "def quick_search(query, top_k=5):\n",
        "    \"\"\"Quick search function - loads DB and searches in one call\"\"\"\n",
        "    db = MedicalVectorDB()\n",
        "    db.load()\n",
        "    return db.search(query, top_k)\n",
        "'''\n",
        "\n",
        "with open('vector_db_utils.py', 'w') as f:\n",
        "    f.write(vector_db_utils_code)\n",
        "\n",
        "print(\" Created vector_db_utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqZw3W94122C",
        "outputId": "291cb1de-c811-40be-e54f-02cbfd9bfdbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMPREHENSIVE VECTOR DATABASE TESTING\n",
            "\n",
            "Testing search speed with 5 queries\n",
            "\n",
            " Performance Results:\n",
            "  Total time: 0.46 seconds\n",
            "  Average time per query: 0.092 seconds\n",
            "  Queries per second: 10.91\n",
            "\n",
            "Testing specialty-specific retrieval accuracy\n",
            "\n",
            "Query: What is the role of insulin in diabetes management?\n",
            "Expected specialty: diabetes\n",
            "Results matching specialty: 5/5 (100%)\n",
            "Top result: Diabetes mellitus: an overview of the types, symptoms, compl...\n",
            "Similarity: 0.9406\n",
            "\n",
            "Query: What are risk factors for myocardial infarction?\n",
            "Expected specialty: cardiology\n",
            "Results matching specialty: 2/5 (40%)\n",
            "Top result: Update on Preventive Cardiology....\n",
            "Similarity: 0.9036\n",
            "\n",
            "Query: How effective are COVID-19 vaccines?\n",
            "Expected specialty: infectious_diseases\n",
            "Results matching specialty: 3/5 (60%)\n",
            "Top result: Why and How Vaccines Work....\n",
            "Similarity: 0.9408\n",
            "\n",
            "COMPLETED VECTOR DATABASE SETUP AND TESTING\n"
          ]
        }
      ],
      "source": [
        "print(\"COMPREHENSIVE VECTOR DATABASE TESTING\")\n",
        "\n",
        "# Testing performance\n",
        "import time\n",
        "test_queries_performance = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How is COVID-19 transmitted?\",\n",
        "    \"What treatments exist for heart failure?\",\n",
        "    \"What causes high blood pressure?\",\n",
        "    \"How do antibiotics work?\"\n",
        "]\n",
        "\n",
        "print(f\"\\nTesting search speed with {len(test_queries_performance)} queries\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for query in test_queries_performance:\n",
        "    results = search_medical_literature(query, index, metadata, model, top_k=5)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "avg_time = total_time / len(test_queries_performance)\n",
        "\n",
        "print(f\"\\n Performance Results:\")\n",
        "print(f\"  Total time: {total_time:.2f} seconds\")\n",
        "print(f\"  Average time per query: {avg_time:.3f} seconds\")\n",
        "print(f\"  Queries per second: {1/avg_time:.2f}\")\n",
        "\n",
        "# Accuracy test\n",
        "relevance_queries = {\n",
        "    \"diabetes\": \"What is the role of insulin in diabetes management?\",\n",
        "    \"cardiology\": \"What are risk factors for myocardial infarction?\",\n",
        "    \"infectious_diseases\": \"How effective are COVID-19 vaccines?\"\n",
        "}\n",
        "\n",
        "print(\"\\nTesting specialty-specific retrieval accuracy\\n\")\n",
        "\n",
        "for expected_specialty, query in relevance_queries.items():\n",
        "    results = search_medical_literature(query, index, metadata, model, top_k=5)\n",
        "\n",
        "    # Count how many results match expected specialty\n",
        "    matching = sum(1 for r in results if r['specialty'] == expected_specialty)\n",
        "    accuracy = (matching / len(results)) * 100\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Expected specialty: {expected_specialty}\")\n",
        "    print(f\"Results matching specialty: {matching}/5 ({accuracy:.0f}%)\")\n",
        "    print(f\"Top result: {results[0]['title'][:60]}...\")\n",
        "    print(f\"Similarity: {results[0]['similarity_score']:.4f}\\n\")\n",
        "\n",
        "print(\"COMPLETED VECTOR DATABASE SETUP AND TESTING\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7nQUfRS-oKa"
      },
      "source": [
        "## **Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/medrag/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from vector_db_utils import MedicalVectorDB\n",
        "from ragpipeline import Medical_RAGPipeline\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvDsSY4mVMSC",
        "outputId": "fab2a772-b74e-4628-cf00-c081382703bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key configured\n"
          ]
        }
      ],
      "source": [
        "# Seting up API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "gemini_api_key = getpass(\"Enter your Gemini API key: \")\n",
        "os.environ['GEMINI_API_KEY'] = gemini_api_key\n",
        "\n",
        "print(\"API key configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading retrieval system from: vector_database/retrieval_system_20251102.pkl\n",
            " Loaded 1954 documents\n",
            " Index type: IndexFlatIP\n",
            "Loading model: pritamdeka/S-PubMedBert-MS-MARCO...\n",
            "Model loaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<vector_db_utils.MedicalVectorDB at 0x311da9090>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#loading vector databases\n",
        "vector_db = MedicalVectorDB(vector_db_dir='vector_database')\n",
        "vector_db.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr6TeSTRVxR5",
        "outputId": "378c5c7e-77e5-44b8-904e-9eb3096db323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Gemini model loaded: gemini-2.5-flash\n",
            " RAG Pipeline initialized with Gemini\n"
          ]
        }
      ],
      "source": [
        "# Initializing RAG Pipeline\n",
        "\n",
        "pipeline = Medical_RAGPipeline(\n",
        "    vector_db=vector_db,\n",
        "    api_key=gemini_api_key,\n",
        "    model='gemini-2.5-flash',\n",
        "    top_k=3,\n",
        "    similarity_threshold=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ueUM6pFmV2tJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----> NO RISKS <-------\n",
            "Query: What are the early symptoms of type 2 diabetes mellitus?\n",
            "1. Retrieving documents\n",
            "   Found 3 relevant documents\n",
            "2. Formatting context\n",
            "3. Generating answer with Gemini\n",
            " Answer generated\n",
            "4. Extracting citations\n",
            "  0 citations found\n",
            "\n",
            " Query #1 \n",
            "\n",
            "Question:\n",
            "What are the early symptoms of type 2 diabetes mellitus?\n",
            "\n",
            "Answer:\n",
            "The provided medical literature does not describe the early symptoms of type 2 diabetes mellitus. The documents discuss the definition, prevalence, pathophysiology, complications, and management of type 2 diabetes, but do not detail its initial clinical presentation or symptoms [PMID: 35192474, PMID: 31736194, PMID: 34319011].\n",
            "\n",
            "Source Documents (3):\n",
            "\n",
            "  1. Uncommon forms of diabetes....\n",
            "     PMID: 35192474 | Similarity: 0.948\n",
            "\n",
            "  2. Insights on the current status and advancement of diabetes mellitus type 2 and t...\n",
            "     PMID: 31736194 | Similarity: 0.938\n",
            "\n",
            "  3. Emerging Targets in Type 2 Diabetes and Diabetic Complications....\n",
            "     PMID: 34319011 | Similarity: 0.937\n",
            "\n",
            "Done!!\n"
          ]
        }
      ],
      "source": [
        "test_query = \"What are the early symptoms of type 2 diabetes mellitus?\"\n",
        "\n",
        "result = pipeline.query(test_query)\n",
        "\n",
        "# Display result\n",
        "pipeline.print_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----> NO RISKS <-------\n",
            "Query:  How is atrial fibrillation treated?\n",
            "1. Retrieving documents\n",
            "   Found 3 relevant documents\n",
            "2. Formatting context\n",
            "3. Generating answer with Gemini\n",
            " Answer generated\n",
            "4. Extracting citations\n",
            "  0 citations found\n",
            "\n",
            " Query #2 \n",
            "\n",
            "Question:\n",
            " How is atrial fibrillation treated?\n",
            "\n",
            "Answer:\n",
            "Error generating answer: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 2.\n",
            "\n",
            "Source Documents (3):\n",
            "\n",
            "  1. The 2024 European Society of Cardiology Guidelines for Diagnosis and Management ...\n",
            "     PMID: 39374908 | Similarity: 0.922\n",
            "\n",
            "  2. Reprint of: Scientific statement from the French neurovascular and cardiac socie...\n",
            "     PMID: 39510937 | Similarity: 0.911\n",
            "\n",
            "  3. Design and deployment of the STEEER-AF trial to evaluate and improve guideline a...\n",
            "     PMID: 38940494 | Similarity: 0.905\n",
            "\n",
            "Done!!\n"
          ]
        }
      ],
      "source": [
        "test_query2 = \" How is atrial fibrillation treated?\"\n",
        "\n",
        "result = pipeline.query(test_query2)\n",
        "\n",
        "# Display result\n",
        "pipeline.print_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TRUSTWORTHINESS REPORT\n",
            "\n",
            "Citation Verification:\n",
            "   Total Citations: 3\n",
            "   Valid Citations: 0\n",
            "   Citation Accuracy: 0.0%\n",
            "   Invalid Citations: ['31736194', '35192474', '34319011']\n",
            "\n",
            "Transparency Score: 60.0% (3/5)\n",
            "Issues: Missing citations, No uncertainty indicators\n",
            "\n",
            "Source Quality:\n",
            "  • Number of sources: 3\n",
            "  • Average similarity: 0.941\n",
            "  • Specialties: diabetes\n",
            "\n",
            "Reproducibility:  All sources verifiable via PMID\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate trustworthiness report\n",
        "trust_report = pipeline.trustworthiness_verifier.generate_trustworthiness_report(result)\n",
        "print(trust_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpD_hxVPw24M"
      },
      "source": [
        "## **MISC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "IAm_zIzFvOwp",
        "outputId": "ef30a67a-6684-47d9-87f3-029abc90b58f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(\"=\"*60)\\nprint(\"MEDICAL RAG SYSTEM - DATA COLLECTION STAGE\")\\nprint(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION\")\\nprint(\"=\"*60)\\n\\n# Load collected data\\ndf = pd.read_csv(\\'medical_literature_dataset.csv\\')\\n\\nprint(f\"\\nLoaded {len(df)} articles from PubMED\\n\")\\n\\n# Step 1: Data Quality Validation\\nquality_report, quality_issues = validate_data_quality(df)\\n\\n# Step 2: Data Cleaning\\ndf_clean = clean_medical_data(df)\\n\\n# Step 3: Bias Detection\\nbias_report, biases = detect_medical_bias(df_clean)\\n\\n# Step 4: Privacy Compliance\\nprivacy_findings, privacy_issues = check_privacy_compliance(df_clean)\\n\\n# Step 5: Representativeness Analysis\\nrep_analysis = analyze_data_representativeness(df_clean)\\n\\n# Step 6: Generate Comprehensive Report\\ngenerate_data_collection_report(\\n    df_clean,\\n    quality_report,\\n    bias_report,\\n    privacy_findings,\\n    rep_analysis\\n)\\n\\n# Save final cleaned dataset\\ndf_clean.to_csv(\\'medical_literature_final.csv\\', index=False)\\nprint(\"✓ Final cleaned dataset saved: medical_literature_final.csv\")\\n\\nprint(\"\\n\" + \"=\"*60)\\nprint(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION COMPLETE\")\\nprint(\"=\"*60)'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "print(\"=\"*60)\n",
        "print(\"MEDICAL RAG SYSTEM - DATA COLLECTION STAGE\")\n",
        "print(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load collected data\n",
        "df = pd.read_csv('medical_literature_dataset.csv')\n",
        "\n",
        "print(f\"\\nLoaded {len(df)} articles from PubMED\\n\")\n",
        "\n",
        "# Step 1: Data Quality Validation\n",
        "quality_report, quality_issues = validate_data_quality(df)\n",
        "\n",
        "# Step 2: Data Cleaning\n",
        "df_clean = clean_medical_data(df)\n",
        "\n",
        "# Step 3: Bias Detection\n",
        "bias_report, biases = detect_medical_bias(df_clean)\n",
        "\n",
        "# Step 4: Privacy Compliance\n",
        "privacy_findings, privacy_issues = check_privacy_compliance(df_clean)\n",
        "\n",
        "# Step 5: Representativeness Analysis\n",
        "rep_analysis = analyze_data_representativeness(df_clean)\n",
        "\n",
        "# Step 6: Generate Comprehensive Report\n",
        "generate_data_collection_report(\n",
        "    df_clean,\n",
        "    quality_report,\n",
        "    bias_report,\n",
        "    privacy_findings,\n",
        "    rep_analysis\n",
        ")\n",
        "\n",
        "# Save final cleaned dataset\n",
        "df_clean.to_csv('medical_literature_final.csv', index=False)\n",
        "print(\"✓ Final cleaned dataset saved: medical_literature_final.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RISK MANAGEMENT & TRUSTWORTHINESS IMPLEMENTATION COMPLETE\")\n",
        "print(\"=\"*60)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YrxyjWzwmeF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medrag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
